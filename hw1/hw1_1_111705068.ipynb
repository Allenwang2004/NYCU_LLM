{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662de657",
   "metadata": {},
   "source": [
    "## N-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "923bf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "class NGramModel:\n",
    "    \"\"\"\n",
    "    N-gram語言模型實現\n",
    "    支援 n=2 (bigram) 和 n=3 (trigram)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n: int = 2):\n",
    "        \"\"\"\n",
    "        初始化N-gram模型\n",
    "        \n",
    "        Args:\n",
    "            n (int): n-gram的階數 (2 for bigram, 3 for trigram)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(int)  # n-gram計數\n",
    "        self.context_counts = defaultdict(int)  # (n-1)-gram計數\n",
    "        self.vocabulary = set()  # 詞彙表\n",
    "        self.total_words = 0\n",
    "        \n",
    "        # 特殊符號\n",
    "        self.start_token = \"<s>\"\n",
    "        self.end_token = \"</s>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        文本預處理\n",
    "        \n",
    "        Args:\n",
    "            text (str): 原始文本\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: 處理後的詞列表\n",
    "        \"\"\"\n",
    "        # 轉小寫，保留字母、數字和基本標點\n",
    "        text = text.lower()\n",
    "        # 用正則表達式分詞\n",
    "        words = re.findall(r'\\b\\w+\\b|[.!?]', text)\n",
    "        return words\n",
    "    \n",
    "    def add_sentence_markers(self, words: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        為句子添加開始和結束標記\n",
    "        \n",
    "        Args:\n",
    "            words (List[str]): 詞列表\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: 添加標記後的詞列表\n",
    "        \"\"\"\n",
    "        # 根據n-gram階數添加適當數量的開始標記\n",
    "        start_markers = [self.start_token] * (self.n - 1)\n",
    "        return start_markers + words + [self.end_token]\n",
    "    \n",
    "    def get_ngrams(self, words: List[str]) -> List[Tuple[str, ...]]:\n",
    "        \"\"\"\n",
    "        從詞列表生成n-gram\n",
    "        \n",
    "        Args:\n",
    "            words (List[str]): 詞列表\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[str, ...]]: n-gram列表\n",
    "        \"\"\"\n",
    "        ngrams = []\n",
    "        for i in range(len(words) - self.n + 1):\n",
    "            ngram = tuple(words[i:i + self.n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "    \n",
    "    def get_contexts(self, words: List[str]) -> List[Tuple[str, ...]]:\n",
    "        \"\"\"\n",
    "        從詞列表生成context (n-1)-gram\n",
    "        \n",
    "        Args:\n",
    "            words (List[str]): 詞列表\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[str, ...]]: context列表\n",
    "        \"\"\"\n",
    "        contexts = []\n",
    "        for i in range(len(words) - self.n + 1):\n",
    "            context = tuple(words[i:i + self.n - 1])\n",
    "            contexts.append(context)\n",
    "        return contexts\n",
    "    \n",
    "    def train(self, train_file: str):\n",
    "        \"\"\"\n",
    "        訓練N-gram模型\n",
    "        \n",
    "        Args:\n",
    "            train_file (str): 訓練文件路徑\n",
    "        \"\"\"\n",
    "        print(f\"Start training {self.n}-gram model...\")\n",
    "        \n",
    "        line_count = 0\n",
    "        with open(train_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                # 預處理文本\n",
    "                words = self.preprocess_text(line)\n",
    "                if len(words) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # 添加句子標記\n",
    "                words_with_markers = self.add_sentence_markers(words)\n",
    "                \n",
    "                # 更新詞彙表\n",
    "                self.vocabulary.update(words)\n",
    "                self.total_words += len(words)\n",
    "                \n",
    "                # 生成n-gram和context\n",
    "                ngrams = self.get_ngrams(words_with_markers)\n",
    "                contexts = self.get_contexts(words_with_markers)\n",
    "                \n",
    "                # 更新計數\n",
    "                for ngram in ngrams:\n",
    "                    self.ngram_counts[ngram] += 1\n",
    "                \n",
    "                for context in contexts:\n",
    "                    self.context_counts[context] += 1\n",
    "\n",
    "        print(f\"Training completed!\")\n",
    "        print(f\"Total words: {self.total_words}\")\n",
    "        print(f\"Vocabulary size: {len(self.vocabulary)}\")\n",
    "        print(f\"N-gram total: {len(self.ngram_counts)}\")\n",
    "        print(f\"Context total: {len(self.context_counts)}\")\n",
    "\n",
    "    def get_probability(self, ngram: Tuple[str, ...]) -> float:\n",
    "        \"\"\"\n",
    "        計算n-gram的條件概率\n",
    "        使用最大似然估計 (MLE)\n",
    "        \n",
    "        Args:\n",
    "            ngram (Tuple[str, ...]): n-gram\n",
    "            \n",
    "        Returns:\n",
    "            float: 條件概率\n",
    "        \"\"\"\n",
    "        if len(ngram) != self.n:\n",
    "            raise ValueError(f\"N-gram長度應為 {self.n}\")\n",
    "        \n",
    "        # 取得context\n",
    "        context = ngram[:-1]\n",
    "        \n",
    "        # 處理未見過的context\n",
    "        if self.context_counts[context] == 0:\n",
    "            return 1e-10  # 平滑處理，避免概率為0\n",
    "        \n",
    "        # P(w_n | w_1, ..., w_{n-1}) = Count(w_1, ..., w_n) / Count(w_1, ..., w_{n-1})\n",
    "        return self.ngram_counts[ngram] / self.context_counts[context]\n",
    "    \n",
    "    def get_sentence_probability(self, sentence: str) -> float:\n",
    "        \"\"\"\n",
    "        計算句子的概率\n",
    "        \n",
    "        Args:\n",
    "            sentence (str): 句子\n",
    "            \n",
    "        Returns:\n",
    "            float: 句子概率的對數值\n",
    "        \"\"\"\n",
    "        words = self.preprocess_text(sentence)\n",
    "        if len(words) == 0:\n",
    "            return float('-inf')\n",
    "        \n",
    "        words_with_markers = self.add_sentence_markers(words)\n",
    "        ngrams = self.get_ngrams(words_with_markers)\n",
    "        \n",
    "        log_prob = 0.0\n",
    "        for ngram in ngrams:\n",
    "            prob = self.get_probability(ngram)\n",
    "            if prob > 0:\n",
    "                log_prob += math.log(prob)\n",
    "            else:\n",
    "                log_prob += math.log(1e-10)  # 平滑處理\n",
    "        \n",
    "        return log_prob\n",
    "    \n",
    "    def calculate_perplexity(self, test_file: str) -> float:\n",
    "        \"\"\"\n",
    "        計算測試集的困惑度 (perplexity)\n",
    "        \n",
    "        Args:\n",
    "            test_file (str): 測試文件路徑\n",
    "            \n",
    "        Returns:\n",
    "            float: 困惑度值\n",
    "        \"\"\"\n",
    "        print(f\"Calculating {self.n}-gram model perplexity...\")\n",
    "        \n",
    "        total_log_prob = 0.0\n",
    "        total_words = 0\n",
    "        line_count = 0\n",
    "        \n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                words = self.preprocess_text(line)\n",
    "                if len(words) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # 計算句子概率\n",
    "                log_prob = self.get_sentence_probability(line)\n",
    "                total_log_prob += log_prob\n",
    "                total_words += len(words)\n",
    "                \n",
    "        \n",
    "        # 計算困惑度: PP = exp(-1/N * sum(log P(sentence)))\n",
    "        avg_log_prob = total_log_prob / total_words\n",
    "        perplexity = math.exp(-avg_log_prob)\n",
    "        \n",
    "        print(f\"測試集總詞數: {total_words}\")\n",
    "        print(f\"平均對數概率: {avg_log_prob:.6f}\")\n",
    "        print(f\"{self.n}-gram 困惑度: {perplexity:.2f}\")\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    def calculate_accuracy(self, test_file: str) -> float:\n",
    "        \"\"\"\n",
    "        計算測試集的準確率 (accuracy)\n",
    "        預測下一個詞的準確率\n",
    "        \n",
    "        Args:\n",
    "            test_file (str): 測試文件路徑\n",
    "            \n",
    "        Returns:\n",
    "            float: 準確率值 (0-1)\n",
    "        \"\"\"\n",
    "        print(f\"Calculating {self.n}-gram model accuracy...\")\n",
    "        \n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            count = 0\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                words = self.preprocess_text(line)\n",
    "                if len(words) < self.n:\n",
    "                    continue\n",
    "                \n",
    "                # 添加句子標記\n",
    "                words_with_markers = self.add_sentence_markers(words)\n",
    "                \n",
    "                # 對每個可能的 n-gram 進行預測\n",
    "                for i in range(self.n - 1, len(words_with_markers)):\n",
    "                    count += 1\n",
    "                    if count > 50:  # 每行最多預測50次\n",
    "                        break\n",
    "                    # 取得 context (前 n-1 個詞)\n",
    "                    context = tuple(words_with_markers[i - self.n + 1:i])\n",
    "                    # 實際的下一個詞\n",
    "                    actual_word = words_with_markers[i]\n",
    "\n",
    "                    # 預測下一個詞 (選擇概率最高的詞)\n",
    "                    predicted_word = self.predict_next_word(context)\n",
    "                    \n",
    "                    if predicted_word == actual_word:\n",
    "                        correct_predictions += 1\n",
    "                    total_predictions += 1\n",
    "                \n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        print(f\"正確預測數: {correct_predictions}\")\n",
    "        print(f\"總預測數: {total_predictions}\")\n",
    "        print(f\"{self.n}-gram 準確率: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def predict_next_word(self, context: Tuple[str, ...]) -> str:\n",
    "        \"\"\"\n",
    "        根據給定的 context 預測下一個最可能的詞\n",
    "        \n",
    "        Args:\n",
    "            context (Tuple[str, ...]): 上下文\n",
    "            \n",
    "        Returns:\n",
    "            str: 預測的下一個詞\n",
    "        \"\"\"\n",
    "        if len(context) != self.n - 1:\n",
    "            raise ValueError(f\"Context 長度應為 {self.n - 1}\")\n",
    "        \n",
    "        # 找出所有以此 context 開始的 n-gram\n",
    "        candidate_ngrams = []\n",
    "        for ngram in self.ngram_counts:\n",
    "            if ngram[:-1] == context:\n",
    "                probability = self.get_probability(ngram)\n",
    "                candidate_ngrams.append((ngram[-1], probability))\n",
    "        \n",
    "        if not candidate_ngrams:\n",
    "            # 如果沒有找到匹配的 n-gram，返回未知詞標記\n",
    "            return self.unk_token\n",
    "        \n",
    "        # 選擇概率最高的詞\n",
    "        best_word = max(candidate_ngrams, key=lambda x: x[1])[0]\n",
    "        return best_word\n",
    "    \n",
    "    def generate_text(self, context: Tuple[str, ...], max_length: int = 20) -> str:\n",
    "        \"\"\"\n",
    "        基於給定context生成文本\n",
    "        \n",
    "        Args:\n",
    "            context (Tuple[str, ...]): 初始context\n",
    "            max_length (int): 最大生成長度\n",
    "            \n",
    "        Returns:\n",
    "            str: 生成的文本\n",
    "        \"\"\"\n",
    "        if len(context) != self.n - 1:\n",
    "            raise ValueError(f\"Context長度應為 {self.n - 1}\")\n",
    "        \n",
    "        result = list(context)\n",
    "        current_context = context\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # 找到所有以current_context開頭的n-gram\n",
    "            candidates = []\n",
    "            for ngram, count in self.ngram_counts.items():\n",
    "                if ngram[:-1] == current_context:\n",
    "                    candidates.extend([ngram[-1]] * count)\n",
    "\n",
    "            if not candidates:\n",
    "                break  # 無法繼續生成\n",
    "            \n",
    "            # 移除結束標記\n",
    "            candidates = [word for word in candidates if word != self.end_token]\n",
    "\n",
    "            # 隨機選擇下一個詞\n",
    "            next_word = random.choice(candidates)\n",
    "            \n",
    "            result.append(next_word)\n",
    "            # 更新context\n",
    "            current_context = tuple(result[-(self.n-1):])\n",
    "        \n",
    "        # 移除開始標記\n",
    "        filtered_result = [word for word in result if word != self.start_token]\n",
    "        return ' '.join(filtered_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8a984",
   "metadata": {},
   "source": [
    "## N-gram training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79697132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "N-gram model Training and Evaluation\n",
      "============================================================\n",
      "\n",
      "==================== N=2 (Bigram) ====================\n",
      "Train 2-gram model...\n",
      "Start training 2-gram model...\n",
      "Training completed!\n",
      "Total words: 17728510\n",
      "Vocabulary size: 38404\n",
      "N-gram total: 660108\n",
      "Context total: 38405\n",
      "Training time: 12.12 seconds\n",
      "Calculating 2-gram model perplexity...\n",
      "測試集總詞數: 4420449\n",
      "平均對數概率: -4.562487\n",
      "2-gram 困惑度: 95.82\n",
      "Calculating 2-gram model accuracy...\n",
      "正確預測數: 15\n",
      "總預測數: 50\n",
      "2-gram 準確率: 0.3000 (30.00%)\n",
      "Testing time: 5.64 seconds\n",
      "Accuracy: 0.3000 (30.00%)\n",
      "\n",
      "2-gram model text generation examples:\n",
      "  Context: add -> add onion and secure with the yoghurt and next grab 1 hour and add remaining ingredients\n",
      "  Context: cook -> cook fettuccini in a food colouring and almond extract in slightly thicken for 10 min or\n",
      "  Context: bake -> bake in whipped cream cheese or plate in a full the pumpkin pie shell and bring\n",
      "\n",
      "==================== N=3 (Trigram) ====================\n",
      "Train 3-gram model...\n",
      "Start training 3-gram model...\n",
      "Training completed!\n",
      "Total words: 17728510\n",
      "Vocabulary size: 38404\n",
      "N-gram total: 2573019\n",
      "Context total: 643488\n",
      "Training time: 16.95 seconds\n",
      "Calculating 3-gram model perplexity...\n",
      "測試集總詞數: 4420449\n",
      "平均對數概率: -5.177542\n",
      "3-gram 困惑度: 177.25\n",
      "Calculating 3-gram model accuracy...\n",
      "正確預測數: 19\n",
      "總預測數: 50\n",
      "3-gram 準確率: 0.3800 (38.00%)\n",
      "Testing time: 10.40 seconds\n",
      "Accuracy: 0.3800 (38.00%)\n",
      "\n",
      "3-gram model text generation examples:\n",
      "  Context: add the -> add the shallots for about 30 minutes of baking time can be served warm either heat it\n",
      "  Context: cook for -> cook for 8 hours or until softened then place remaining cake layer and cook until onion and\n",
      "  Error occurred during text generation: Cannot choose from an empty sequence\n",
      "\n",
      "============================================================\n",
      "Results Comparison\n",
      "============================================================\n",
      "Metric               Bigram (n=2)    Trigram (n=3)   Difference     \n",
      "-----------------------------------------------------------------\n",
      "Perplexity           95.82           177.25          +84.98%\n",
      "Accuracy             0.3000          0.3800          +26.67%\n",
      "Training time        12.12           16.95           +39.81%\n",
      "N-gram types         742,015         2,896,633       +290.37%\n",
      "Vocabulary size      38,404          38,404          Same           \n",
      "Total words          17,728,510      17,728,510      Same           \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "N-gram語言模型訓練和測試程式\n",
    "使用 train.txt 訓練模型，在 test.txt 上評估性能\n",
    "比較 n=2 (bigram) 和 n=3 (trigram) 的表現\n",
    "\"\"\"\n",
    "\n",
    "train_file = \"data/train.txt\"\n",
    "test_file = \"data/test.txt\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"N-gram model Training and Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = {}\n",
    "trained_models = {}  # Store trained models for text generation\n",
    "\n",
    "for n in [2, 3]:\n",
    "    print(f\"\\n{'='*20} N={n} ({'Bigram' if n==2 else 'Trigram'}) {'='*20}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    model = NGramModel(n=n)\n",
    "\n",
    "    print(f\"Train {n}-gram model...\")\n",
    "    model.train(train_file)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # Calculate perplexity and accuracy\n",
    "    start_time = time.time()\n",
    "    perplexity = model.calculate_perplexity(test_file)\n",
    "    accuracy = model.calculate_accuracy(test_file)\n",
    "    test_time = time.time() - start_time\n",
    "\n",
    "    print(f\"Testing time: {test_time:.2f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # save results and model\n",
    "    results[n] = {\n",
    "        'perplexity': perplexity,\n",
    "        'accuracy': accuracy,\n",
    "        'training_time': training_time,\n",
    "        'test_time': test_time,\n",
    "        'vocab_size': len(model.vocabulary),\n",
    "        'total_words': model.total_words,\n",
    "        'ngram_types': len(model.ngram_counts),\n",
    "        'context_types': len(model.context_counts)\n",
    "    }\n",
    "    trained_models[n] = model  # Store the trained model\n",
    "\n",
    "    print(f\"\\n{n}-gram model text generation examples:\")\n",
    "    try:\n",
    "        if n == 2:\n",
    "            contexts = [(\"add\",), (\"cook\",), (\"bake\",)]\n",
    "        else:\n",
    "            contexts = [(\"add\", \"the\"), (\"cook\", \"for\"), (\"bake\", \"at\")]\n",
    "        \n",
    "        for context in contexts:\n",
    "            generated = model.generate_text(context, max_length=15)\n",
    "            print(f\"  Context: {' '.join(context)} -> {generated}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error occurred during text generation: {e}\")\n",
    "\n",
    "# Results comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Results Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"{'Metric':<20} {'Bigram (n=2)':<15} {'Trigram (n=3)':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "bigram_pp = results[2]['perplexity']\n",
    "trigram_pp = results[3]['perplexity']\n",
    "pp_diff = ((trigram_pp - bigram_pp) / bigram_pp) * 100\n",
    "\n",
    "print(f\"{'Perplexity':<20} {bigram_pp:<15.2f} {trigram_pp:<15.2f} {pp_diff:+.2f}%\")\n",
    "\n",
    "bigram_acc = results[2]['accuracy']\n",
    "trigram_acc = results[3]['accuracy']\n",
    "acc_diff = ((trigram_acc - bigram_acc) / bigram_acc) * 100\n",
    "\n",
    "print(f\"{'Accuracy':<20} {bigram_acc:<15.4f} {trigram_acc:<15.4f} {acc_diff:+.2f}%\")\n",
    "\n",
    "bigram_time = results[2]['training_time']\n",
    "trigram_time = results[3]['training_time']\n",
    "time_diff = ((trigram_time - bigram_time) / bigram_time) * 100\n",
    "\n",
    "print(f\"{'Training time':<20} {bigram_time:<15.2f} {trigram_time:<15.2f} {time_diff:+.2f}%\")\n",
    "\n",
    "bigram_ngrams = results[2]['ngram_types']  \n",
    "trigram_ngrams = results[3]['ngram_types']\n",
    "ngram_diff = ((trigram_ngrams - bigram_ngrams) / bigram_ngrams) * 100\n",
    "\n",
    "print(f\"{'N-gram types':<20} {bigram_ngrams:<15,} {trigram_ngrams:<15,} {ngram_diff:+.2f}%\")\n",
    "\n",
    "print(f\"{'Vocabulary size':<20} {results[2]['vocab_size']:<15,} {results[3]['vocab_size']:<15,} {'Same':<15}\")\n",
    "print(f\"{'Total words':<20} {results[2]['total_words']:<15,} {results[3]['total_words']:<15,} {'Same':<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829fd50",
   "metadata": {},
   "source": [
    "### Question: Use n= 2 and n= 3 to calculate bigrams and trigrams, respectively, and evaluatethe test accuracy in test.txt and make some discussion.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147730c5",
   "metadata": {},
   "source": [
    "### Question: Please observe the hardware usage in your computer while the models are running and make some discussion.\n",
    "\n",
    "Answer: When training the 2-gram model, the process only took about 1.0% of memory, while on the other hand, 3-gram model need 5.7% of memory. The reason is probably because 3-gram model has much more N-gram types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f242a24",
   "metadata": {},
   "source": [
    "## Complete the incomplete.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb009aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'cover with' -> 'cover with aluminum foil and continue to set them on the pot to high and are ready to serve this with frozen'\n",
      "  'roll up' -> 'roll up a day ahead and kept chilled in an 8x11 inch baking pan with cooking spray in an oven tray brushed'\n",
      "  'cook the' -> 'cook the spinach mixture over dough rectangles with cheese and mint along the edge of the pan so nothing gets cold !'\n",
      "  'stir in' -> 'stir in creamed mixture just enough to handle the dough from greased slotted spoon to scrape out the insides into a greased'\n",
      "  'spread out' -> 'spread out on flat ends on kitchen paper and butter until creamy and soft scrambled eggs into the center comes out clean'\n",
      "  'transfer the' -> 'transfer the pan with parchment paper or in a smaller bowl combine flour with a fork slightly puncture the peppers to a'\n",
      "  'put the' -> 'put the cheese melts and sugar on the rim of the ricotta mix on high until sugar is dissolved and the onions'\n",
      "  'push the' -> 'push the batter will be thick enough to maintain a chocolate wafer crust and return to a boil over medium heat until'\n",
      "  'cut into' -> 'cut into squares or use your favourite garnish and serve slightly warm milk into the rice in a small mixing bowl and'\n",
      "  'toss the' -> 'toss the salad greens between 6 bowls and serve with the back of a double boiler 3 squares semi sweet chocolate in'\n"
     ]
    }
   ],
   "source": [
    "# Test with incomplete.txt for text completion\n",
    "with open(\"data/incomplete.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    incomplete_lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "for i, incomplete_text in enumerate(incomplete_lines):\n",
    "    words = model.preprocess_text(incomplete_text)\n",
    "    context = (words[-2], words[-1])\n",
    "    # Generate completion\n",
    "    completion = model.generate_text(context, max_length=20)\n",
    "    # Remove the context words from completion to show only new words\n",
    "    context_str = ' '.join(context)\n",
    "    new_words = completion[len(context_str):].strip()    \n",
    "    full_completion = incomplete_text + \" \" + new_words\n",
    "    print(f\"  '{incomplete_text}' -> '{full_completion}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56aa64",
   "metadata": {},
   "source": [
    "# RNN and LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee984be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b718c8d",
   "metadata": {},
   "source": [
    "## Vocabulary and TextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa2bfe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"詞彙表類別，處理詞彙到索引的轉換\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_count = Counter()\n",
    "        \n",
    "        # 特殊標記\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.start_token = '<START>'\n",
    "        self.end_token = '<END>'\n",
    "        \n",
    "        # 初始化特殊標記\n",
    "        self.add_word(self.pad_token)\n",
    "        self.add_word(self.unk_token)\n",
    "        self.add_word(self.start_token)\n",
    "        self.add_word(self.end_token)\n",
    "        \n",
    "    def add_word(self, word: str) -> int:\n",
    "        \"\"\"添加詞彙到詞彙表\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "        self.word_count[word] += 1\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 2):\n",
    "        \"\"\"建立詞彙表\"\"\"\n",
    "        print(\"Building vocabulary...\")\n",
    "        \n",
    "        # 統計詞頻\n",
    "        for text in texts:\n",
    "            words = self.preprocess_text(text)\n",
    "            for word in words:\n",
    "                self.word_count[word] += 1\n",
    "        \n",
    "        # 添加高頻詞到詞彙表\n",
    "        for word, count in self.word_count.items():\n",
    "            if count >= min_freq and word not in self.word2idx:\n",
    "                self.add_word(word)\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "        print(f\"Most common words: {self.word_count.most_common(10)}\")\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"文本預處理\"\"\"\n",
    "        text = text.lower().strip()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return words\n",
    "    \n",
    "    def text_to_indices(self, text: str) -> List[int]:\n",
    "        \"\"\"將文本轉換為索引序列\"\"\"\n",
    "        words = self.preprocess_text(text)\n",
    "        indices = [self.word2idx[self.start_token]]\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.word2idx:\n",
    "                indices.append(self.word2idx[word])\n",
    "            else:\n",
    "                indices.append(self.word2idx[self.unk_token])\n",
    "        \n",
    "        indices.append(self.word2idx[self.end_token])\n",
    "        return indices\n",
    "    \n",
    "    def indices_to_text(self, indices: List[int]) -> str:\n",
    "        \"\"\"將索引序列轉換為文本\"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            if idx in self.idx2word:\n",
    "                word = self.idx2word[idx]\n",
    "                if word not in [self.pad_token, self.start_token, self.end_token]:\n",
    "                    words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"文本數據集類別\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], vocab: Vocabulary, seq_length: int = 50):\n",
    "        self.vocab = vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.sequences = []\n",
    "        \n",
    "        self.prepare_sequences(texts)\n",
    "    \n",
    "    def prepare_sequences(self, texts: List[str]):\n",
    "        \"\"\"準備訓練序列\"\"\"\n",
    "        print(\"Preparing training sequences...\")\n",
    "        \n",
    "        for text in texts:\n",
    "            indices = self.vocab.text_to_indices(text)\n",
    "            \n",
    "            # 如果序列太短，跳過\n",
    "            if len(indices) < 2:\n",
    "                continue\n",
    "\n",
    "            # 限制總 sequence 數量\n",
    "            if len(self.sequences) >= 1000000:\n",
    "                break\n",
    "            \n",
    "            # 創建滑動窗口序列\n",
    "            for i in range(len(indices) - 1):\n",
    "                # 輸入序列和目標序列\n",
    "                input_seq = indices[max(0, i - self.seq_length + 1):i + 1]\n",
    "                target = indices[i + 1]\n",
    "                \n",
    "                # 填充到固定長度\n",
    "                if len(input_seq) < self.seq_length:\n",
    "                    padding = [self.vocab.word2idx[self.vocab.pad_token]] * (self.seq_length - len(input_seq))\n",
    "                    input_seq = padding + input_seq\n",
    "                \n",
    "                self.sequences.append((input_seq, target))\n",
    "        \n",
    "        print(f\"Created {len(self.sequences)} training sequences\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target = self.sequences[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77e707",
   "metadata": {},
   "source": [
    "## RNN LanguageModel and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c466d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"RNN 語言模型\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 128, \n",
    "                 num_layers: int = 2, dropout: float = 0.2):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 詞嵌入層\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # RNN 層\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, \n",
    "                         batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Dropout 層\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 輸出層\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # 初始化權重\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化模型權重\"\"\"\n",
    "        init_range = 0.1\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-init_range, init_range)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"前向傳播\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 詞嵌入\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # RNN\n",
    "        rnn_out, hidden = self.rnn(embedded, hidden)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 取最後一個時間步的輸出\n",
    "        last_output = rnn_out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        output = self.dropout(last_output)\n",
    "        \n",
    "        # 線性層\n",
    "        output = self.linear(output)  # (batch_size, vocab_size)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"初始化隱藏狀態\"\"\"\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "\n",
    "class RNNTrainer:\n",
    "    \"\"\"RNN 訓練器\"\"\"\n",
    "    \n",
    "    def __init__(self, model, vocab, device, learning_rate: float = 0.001):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        \n",
    "        # 損失函數和優化器\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # 追蹤訓練歷史\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        \n",
    "    def train_epoch(self, dataloader):\n",
    "        \"\"\"訓練一個 epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        # 創建 tqdm 進度條\n",
    "        pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(pbar):\n",
    "            data, targets = data.to(self.device), targets.to(self.device)\n",
    "            batch_size = data.size(0)\n",
    "            \n",
    "            # 初始化隱藏狀態\n",
    "            hidden = self.model.init_hidden(batch_size, self.device)\n",
    "            \n",
    "            # 前向傳播\n",
    "            outputs, _ = self.model(data, hidden)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # 計算準確率\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            \n",
    "            # 反向傳播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            # 更新進度條\n",
    "            current_loss = total_loss / total_samples\n",
    "            current_acc = correct_predictions / total_samples\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{current_loss:.4f}',\n",
    "                'Acc': f'{current_acc:.4f}'\n",
    "            })\n",
    "\n",
    "        \n",
    "        avg_loss = total_loss / total_samples\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        \n",
    "        self.train_losses.append(avg_loss)\n",
    "        self.train_accuracies.append(accuracy)\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \"\"\"評估模型\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        # 創建 tqdm 進度條用於評估\n",
    "        pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in pbar:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                batch_size = data.size(0)\n",
    "                \n",
    "                hidden = self.model.init_hidden(batch_size, self.device)\n",
    "                outputs, _ = self.model(data, hidden)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                # 計算準確率\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct_predictions += (predicted == targets).sum().item()\n",
    "                \n",
    "                total_loss += loss.item() * batch_size\n",
    "                total_samples += batch_size\n",
    "                \n",
    "                # 更新進度條\n",
    "                current_loss = total_loss / total_samples\n",
    "                current_acc = correct_predictions / total_samples\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{current_loss:.4f}',\n",
    "                    'Acc': f'{current_acc:.4f}'\n",
    "                })\n",
    "        \n",
    "        avg_loss = total_loss / total_samples\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        \n",
    "        return avg_loss, accuracy, perplexity\n",
    "    \n",
    "    def generate_text(self, start_text: str, max_length: int = 50, temperature: float = 1.0):\n",
    "        \"\"\"生成文本\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 預處理起始文本\n",
    "        indices = self.vocab.text_to_indices(start_text)\n",
    "        if len(indices) == 0:\n",
    "            indices = [self.vocab.word2idx[self.vocab.start_token]]\n",
    "        \n",
    "        generated = indices.copy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                # 準備輸入序列\n",
    "                input_seq = generated[-50:]  # 取最後50個詞作為上下文\n",
    "                if len(input_seq) < 50:\n",
    "                    padding = [self.vocab.word2idx[self.vocab.pad_token]] * (50 - len(input_seq))\n",
    "                    input_seq = padding + input_seq\n",
    "                \n",
    "                input_tensor = torch.tensor([input_seq], dtype=torch.long).to(self.device)\n",
    "                hidden = self.model.init_hidden(1, self.device)\n",
    "                \n",
    "                # 預測下一個詞\n",
    "                outputs, _ = self.model(input_tensor, hidden)\n",
    "                outputs = outputs / temperature\n",
    "                probabilities = torch.softmax(outputs, dim=-1)\n",
    "                \n",
    "                # 隨機採樣\n",
    "                next_word_idx = torch.multinomial(probabilities, 1).item()\n",
    "                \n",
    "                # 如果生成結束標記，停止生成\n",
    "                if next_word_idx == self.vocab.word2idx[self.vocab.end_token]:\n",
    "                    break\n",
    "                \n",
    "                generated.append(next_word_idx)\n",
    "        \n",
    "        return self.vocab.indices_to_text(generated)\n",
    "    \n",
    "    def plot_learning_curves(self, save_path='rnn_learning_curves.png'):\n",
    "        \"\"\"繪製學習曲線\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # 繪製訓練損失\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        ax1.plot(epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "        ax1.set_title('Training Loss Curve')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # 繪製訓練準確率\n",
    "        ax2.plot(epochs, self.train_accuracies, 'r-', label='Training Accuracy')\n",
    "        ax2.set_title('Training Accuracy Curve')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Learning curves saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8363fd5",
   "metadata": {},
   "source": [
    "## LSTM Language Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d6980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    \"\"\"LSTM 語言模型\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 128, \n",
    "                 num_layers: int = 2, dropout: float = 0.2):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 詞嵌入層\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # LSTM 層\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Dropout 層\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 輸出層\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # 初始化權重\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化模型權重\"\"\"\n",
    "        init_range = 0.1\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-init_range, init_range)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"前向傳播\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 詞嵌入\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 取最後一個時間步的輸出\n",
    "        last_output = lstm_out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        output = self.dropout(last_output)\n",
    "        \n",
    "        # 線性層\n",
    "        output = self.linear(output)  # (batch_size, vocab_size)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"初始化隱藏狀態和細胞狀態\"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "class LSTMTrainer:\n",
    "    \"\"\"LSTM 訓練器\"\"\"\n",
    "    \n",
    "    def __init__(self, model, vocab, device):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        \n",
    "        # 損失函數和優化器\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # 追蹤訓練歷史\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        \n",
    "    def train_epoch(self, dataloader):\n",
    "        \"\"\"訓練一個 epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        # 創建 tqdm 進度條\n",
    "        pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(pbar):\n",
    "            data, targets = data.to(self.device), targets.to(self.device)\n",
    "            batch_size = data.size(0)\n",
    "            \n",
    "            # 初始化隱藏狀態\n",
    "            hidden = self.model.init_hidden(batch_size, self.device)\n",
    "            \n",
    "            # 前向傳播\n",
    "            outputs, _ = self.model(data, hidden)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # 計算準確率\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            \n",
    "            # 反向傳播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            # 更新進度條\n",
    "            current_loss = total_loss / total_samples\n",
    "            current_acc = correct_predictions / total_samples\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{current_loss:.4f}',\n",
    "                'Acc': f'{current_acc:.4f}'\n",
    "            })\n",
    "        \n",
    "        avg_loss = total_loss / total_samples\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        \n",
    "        # 記錄訓練歷史\n",
    "        self.train_losses.append(avg_loss)\n",
    "        self.train_accuracies.append(accuracy)\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \"\"\"評估模型\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        # 創建 tqdm 進度條用於評估\n",
    "        pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in pbar:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                batch_size = data.size(0)\n",
    "                \n",
    "                hidden = self.model.init_hidden(batch_size, self.device)\n",
    "                outputs, _ = self.model(data, hidden)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                # 計算準確率\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct_predictions += (predicted == targets).sum().item()\n",
    "                \n",
    "                total_loss += loss.item() * batch_size\n",
    "                total_samples += batch_size\n",
    "                \n",
    "                # 更新進度條\n",
    "                current_loss = total_loss / total_samples\n",
    "                current_acc = correct_predictions / total_samples\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{current_loss:.4f}',\n",
    "                    'Acc': f'{current_acc:.4f}'\n",
    "                })\n",
    "        \n",
    "        avg_loss = total_loss / total_samples\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        \n",
    "        return avg_loss, accuracy, perplexity\n",
    "    \n",
    "    def generate_text(self, start_text: str, max_length: int = 50, temperature: float = 1.0):\n",
    "        \"\"\"生成文本\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 預處理起始文本\n",
    "        indices = self.vocab.text_to_indices(start_text)\n",
    "        if len(indices) == 0:\n",
    "            indices = [self.vocab.word2idx[self.vocab.start_token]]\n",
    "        \n",
    "        generated = indices.copy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                # 準備輸入序列\n",
    "                input_seq = generated[-50:]  # 取最後50個詞作為上下文\n",
    "                if len(input_seq) < 50:\n",
    "                    padding = [self.vocab.word2idx[self.vocab.pad_token]] * (50 - len(input_seq))\n",
    "                    input_seq = padding + input_seq\n",
    "                \n",
    "                input_tensor = torch.tensor([input_seq], dtype=torch.long).to(self.device)\n",
    "                hidden = self.model.init_hidden(1, self.device)\n",
    "                \n",
    "                # 預測下一個詞\n",
    "                outputs, _ = self.model(input_tensor, hidden)\n",
    "                outputs = outputs / temperature\n",
    "                probabilities = torch.softmax(outputs, dim=-1)\n",
    "                \n",
    "                # 隨機採樣\n",
    "                next_word_idx = torch.multinomial(probabilities, 1).item()\n",
    "                \n",
    "                # 如果生成結束標記，停止生成\n",
    "                if next_word_idx == self.vocab.word2idx[self.vocab.end_token]:\n",
    "                    break\n",
    "                \n",
    "                generated.append(next_word_idx)\n",
    "        \n",
    "        return self.vocab.indices_to_text(generated)\n",
    "    \n",
    "    def plot_learning_curves(self, save_path='lstm_learning_curves.png'):\n",
    "        \"\"\"繪製學習曲線\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # 繪製訓練損失\n",
    "        epochs = range(1, len(self.train_losses) + 1)\n",
    "        ax1.plot(epochs, self.train_losses, 'b-', label='Training Loss')\n",
    "        ax1.set_title('LSTM Training Loss Curve')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # 繪製訓練準確率\n",
    "        ax2.plot(epochs, self.train_accuracies, 'r-', label='Training Accuracy')\n",
    "        ax2.set_title('LSTM Training Accuracy Curve')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Learning curves saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4b8aba",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c371c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> List[str]:\n",
    "    \"\"\"載入訓練數據\"\"\"\n",
    "    texts = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                texts.append(line)\n",
    "    \n",
    "    print(f\"Loaded {len(texts)} texts from {file_path}\")\n",
    "    return texts\n",
    "\n",
    "def evaluate_on_test_data(trainer, vocab, test_file: str, seq_length: int = 50, batch_size: int = 32):\n",
    "    \"\"\"在測試數據上評估模型\"\"\"\n",
    "    print(f\"Evaluating model on {test_file}...\")\n",
    "    \n",
    "    # 載入測試數據\n",
    "    test_texts = load_data(test_file)\n",
    "    if not test_texts:\n",
    "        return\n",
    "    \n",
    "    # 創建測試數據集\n",
    "    test_dataset = TextDataset(test_texts, vocab, seq_length)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # 評估模型\n",
    "    test_loss, test_accuracy, test_perplexity = trainer.evaluate(test_dataloader)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RNN 模型測試結果\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"測試損失 (Test Loss): {test_loss:.4f}\")\n",
    "    print(f\"測試準確率 (Test Accuracy): {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(f\"測試困惑度 (Test Perplexity): {test_perplexity:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return test_loss, test_accuracy, test_perplexity\n",
    "\n",
    "def test_incomplete_sentences(trainer, incomplete_file: str):\n",
    "    \"\"\"測試不完整句子的補全\"\"\"\n",
    "    print(\"Testing incomplete sentence completion...\")\n",
    "    \n",
    "    with open(incomplete_file, 'r', encoding='utf-8') as f:\n",
    "        incomplete_texts = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RNN 模型文本補全結果\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for incomplete_text in incomplete_texts:\n",
    "        completed = trainer.generate_text(incomplete_text, max_length=20, temperature=0.8)\n",
    "        print(f\"輸入: {incomplete_text}\")\n",
    "        print(f\"補全: {completed}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ec481",
   "metadata": {},
   "source": [
    "## RNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b5d6b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 2599668 texts from data/train.txt\n",
      "Building vocabulary...\n",
      "Vocabulary size: 15843\n",
      "Most common words: [('and', 910902), ('the', 851351), ('in', 479094), ('a', 463682), ('to', 460822), ('with', 307128), ('until', 276529), ('add', 252377), ('minutes', 233439), ('of', 231635)]\n",
      "Preparing training sequences...\n",
      "Created 1000002 training sequences\n",
      "Model parameters: 4,137,699\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RNN Training Progress:   0%|          | 0/10 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epoch_pbar:\n\u001b[32m     42\u001b[39m     start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     train_loss, train_accuracy = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     epoch_time = time.time() - start_time\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# 更新 epoch 進度條\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 99\u001b[39m, in \u001b[36mRNNTrainer.train_epoch\u001b[39m\u001b[34m(self, dataloader)\u001b[39m\n\u001b[32m     96\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(outputs, targets)\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# 計算準確率\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m _, predicted = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m correct_predictions += (predicted == targets).sum().item()\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# 反向傳播\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 50\n",
    "\n",
    "# Load data\n",
    "train_texts = load_data('data/train.txt')\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(train_texts, min_freq=3)\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = TextDataset(train_texts, vocab, SEQ_LENGTH)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = RNNLanguageModel(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=HIDDEN_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# create trainer\n",
    "trainer = RNNTrainer(model, vocab, device, LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# 創建 epoch 進度條\n",
    "epoch_pbar = tqdm(range(NUM_EPOCHS), desc=\"RNN Training Progress\")\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_accuracy = trainer.train_epoch(train_dataloader)\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # 更新 epoch 進度條\n",
    "    epoch_pbar.set_postfix({\n",
    "        'Loss': f'{train_loss:.4f}',\n",
    "        'Acc': f'{train_accuracy:.4f}',\n",
    "        'Time': f'{epoch_time:.1f}s'\n",
    "    })\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Time: {epoch_time:.2f}s')\n",
    "    \n",
    "    # 每幾個 epoch 生成一些文本\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        print(f\"\\nEpoch {epoch+1} 生成範例:\")\n",
    "        sample_text = trainer.generate_text(\"add salt\", max_length=15)\n",
    "        print(f\"Generated: {sample_text}\")\n",
    "\n",
    "# save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab': vocab,\n",
    "    'hyperparameters': {\n",
    "        'vocab_size': len(vocab),\n",
    "        'embed_dim': HIDDEN_DIM,\n",
    "        'hidden_dim': HIDDEN_DIM,\n",
    "        'num_layers': NUM_LAYERS\n",
    "    }\n",
    "}, 'rnn_model.pth')\n",
    "\n",
    "print(\"Model saved to rnn_model.pth\")\n",
    "\n",
    "# plot learning curves\n",
    "trainer.plot_learning_curves('rnn_learning_curves.png')\n",
    "\n",
    "# evaluate on test data\n",
    "evaluate_on_test_data(trainer, vocab, 'data/test.txt', SEQ_LENGTH, BATCH_SIZE)\n",
    "\n",
    "# test incomplete sentences\n",
    "test_incomplete_sentences(trainer, 'data/incomplete.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b681ed",
   "metadata": {},
   "source": [
    "## LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97008c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 超參數\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LENGTH = 50\n",
    "\n",
    "# 載入數據\n",
    "train_texts = load_data('data/train.txt')\n",
    "\n",
    "# 建立詞彙表\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(train_texts, min_freq=3)\n",
    "\n",
    "# 創建數據集\n",
    "train_dataset = TextDataset(train_texts, vocab, SEQ_LENGTH)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 創建模型\n",
    "model = LSTMLanguageModel(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=HIDDEN_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# 創建訓練器\n",
    "trainer = LSTMTrainer(model, vocab, device)\n",
    "\n",
    "# 訓練模型\n",
    "print(\"Starting training...\")\n",
    "\n",
    "# 創建 epoch 進度條\n",
    "epoch_pbar = tqdm(range(NUM_EPOCHS), desc=\"LSTM Training Progress\")\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_accuracy = trainer.train_epoch(train_dataloader)\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    # 更新 epoch 進度條\n",
    "    epoch_pbar.set_postfix({\n",
    "        'Loss': f'{train_loss:.4f}',\n",
    "        'Acc': f'{train_accuracy:.4f}',\n",
    "        'Time': f'{epoch_time:.1f}s'\n",
    "    })\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Time: {epoch_time:.2f}s')\n",
    "    \n",
    "\n",
    "# 保存模型\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'vocab': vocab,\n",
    "    'hyperparameters': {\n",
    "        'vocab_size': len(vocab),\n",
    "        'embed_dim': HIDDEN_DIM,\n",
    "        'hidden_dim': HIDDEN_DIM,\n",
    "        'num_layers': NUM_LAYERS\n",
    "    }\n",
    "}, 'lstm_model.pth')\n",
    "\n",
    "print(\"Model saved to lstm_model.pth\")\n",
    "\n",
    "# 繪製學習曲線\n",
    "trainer.plot_learning_curves('lstm_learning_curves.png')\n",
    "\n",
    "# 在測試數據上評估模型\n",
    "evaluate_on_test_data(trainer, vocab, 'data/test.txt', SEQ_LENGTH, BATCH_SIZE)\n",
    "\n",
    "# 測試不完整句子補全\n",
    "test_incomplete_sentences(trainer, 'data/incomplete.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6439dd7d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
