{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923bf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict\n",
    "import logging\n",
    "\n",
    "class NGramModel:\n",
    "    \"\"\"\n",
    "    N-gram語言模型實現\n",
    "    支援 n=2 (bigram) 和 n=3 (trigram)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n: int = 2):\n",
    "        \"\"\"\n",
    "        初始化N-gram模型\n",
    "        \n",
    "        Args:\n",
    "            n (int): n-gram的階數 (2 for bigram, 3 for trigram)\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(int)  # n-gram計數\n",
    "        self.context_counts = defaultdict(int)  # (n-1)-gram計數\n",
    "        self.vocabulary = set()  # 詞彙表\n",
    "        self.total_words = 0\n",
    "        \n",
    "        # 特殊符號\n",
    "        self.start_token = \"<s>\"\n",
    "        self.end_token = \"</s>\"\n",
    "        self.unk_token = \"<unk>\"\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        文本預處理\n",
    "        \n",
    "        Args:\n",
    "            text (str): 原始文本\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: 處理後的詞列表\n",
    "        \"\"\"\n",
    "        # 轉小寫，保留字母、數字和基本標點\n",
    "        text = text.lower()\n",
    "        # 用正則表達式分詞\n",
    "        words = re.findall(r'\\b\\w+\\b|[.!?]', text)\n",
    "        return words\n",
    "    \n",
    "    def add_sentence_markers(self, words: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        為句子添加開始和結束標記\n",
    "        \n",
    "        Args:\n",
    "            words (List[str]): 詞列表\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: 添加標記後的詞列表\n",
    "        \"\"\"\n",
    "        # 根據n-gram階數添加適當數量的開始標記\n",
    "        start_markers = [self.start_token] * (self.n - 1)\n",
    "        return start_markers + words + [self.end_token]\n",
    "    \n",
    "    def get_ngrams(self, words: List[str]) -> List[Tuple[str, ...]]:\n",
    "        \"\"\"\n",
    "        從詞列表生成n-gram\n",
    "        \n",
    "        Args:\n",
    "            words (List[str]): 詞列表\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[str, ...]]: n-gram列表\n",
    "        \"\"\"\n",
    "        ngrams = []\n",
    "        for i in range(len(words) - self.n + 1):\n",
    "            ngram = tuple(words[i:i + self.n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "    \n",
    "    def get_contexts(self, words: List[str]) -> List[Tuple[str, ...]]:\n",
    "        \"\"\"\n",
    "        從詞列表生成context (n-1)-gram\n",
    "        \n",
    "        Args:\n",
    "            words (List[str]): 詞列表\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[str, ...]]: context列表\n",
    "        \"\"\"\n",
    "        contexts = []\n",
    "        for i in range(len(words) - self.n + 1):\n",
    "            context = tuple(words[i:i + self.n - 1])\n",
    "            contexts.append(context)\n",
    "        return contexts\n",
    "    \n",
    "    def train(self, train_file: str):\n",
    "        \"\"\"\n",
    "        訓練N-gram模型\n",
    "        \n",
    "        Args:\n",
    "            train_file (str): 訓練文件路徑\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Start training {self.n}-gram model...\")\n",
    "        \n",
    "        line_count = 0\n",
    "        with open(train_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                # 預處理文本\n",
    "                words = self.preprocess_text(line)\n",
    "                if len(words) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # 添加句子標記\n",
    "                words_with_markers = self.add_sentence_markers(words)\n",
    "                \n",
    "                # 更新詞彙表\n",
    "                self.vocabulary.update(words)\n",
    "                self.total_words += len(words)\n",
    "                \n",
    "                # 生成n-gram和context\n",
    "                ngrams = self.get_ngrams(words_with_markers)\n",
    "                contexts = self.get_contexts(words_with_markers)\n",
    "                \n",
    "                # 更新計數\n",
    "                for ngram in ngrams:\n",
    "                    self.ngram_counts[ngram] += 1\n",
    "                \n",
    "                for context in contexts:\n",
    "                    self.context_counts[context] += 1\n",
    "                \n",
    "                line_count += 1\n",
    "                if line_count % 10000 == 0:\n",
    "                    self.logger.info(f\"Already processed {line_count} lines...\")\n",
    "\n",
    "        self.logger.info(f\"Training completed!\")\n",
    "        self.logger.info(f\"Total words: {self.total_words}\")\n",
    "        self.logger.info(f\"Vocabulary size: {len(self.vocabulary)}\")\n",
    "        self.logger.info(f\"N-gram total: {len(self.ngram_counts)}\")\n",
    "        self.logger.info(f\"Context total: {len(self.context_counts)}\")\n",
    "\n",
    "    def get_probability(self, ngram: Tuple[str, ...]) -> float:\n",
    "        \"\"\"\n",
    "        計算n-gram的條件概率\n",
    "        使用最大似然估計 (MLE)\n",
    "        \n",
    "        Args:\n",
    "            ngram (Tuple[str, ...]): n-gram\n",
    "            \n",
    "        Returns:\n",
    "            float: 條件概率\n",
    "        \"\"\"\n",
    "        if len(ngram) != self.n:\n",
    "            raise ValueError(f\"N-gram長度應為 {self.n}\")\n",
    "        \n",
    "        # 取得context\n",
    "        context = ngram[:-1]\n",
    "        \n",
    "        # 處理未見過的context\n",
    "        if self.context_counts[context] == 0:\n",
    "            return 1e-10  # 平滑處理，避免概率為0\n",
    "        \n",
    "        # P(w_n | w_1, ..., w_{n-1}) = Count(w_1, ..., w_n) / Count(w_1, ..., w_{n-1})\n",
    "        return self.ngram_counts[ngram] / self.context_counts[context]\n",
    "    \n",
    "    def get_sentence_probability(self, sentence: str) -> float:\n",
    "        \"\"\"\n",
    "        計算句子的概率\n",
    "        \n",
    "        Args:\n",
    "            sentence (str): 句子\n",
    "            \n",
    "        Returns:\n",
    "            float: 句子概率的對數值\n",
    "        \"\"\"\n",
    "        words = self.preprocess_text(sentence)\n",
    "        if len(words) == 0:\n",
    "            return float('-inf')\n",
    "        \n",
    "        words_with_markers = self.add_sentence_markers(words)\n",
    "        ngrams = self.get_ngrams(words_with_markers)\n",
    "        \n",
    "        log_prob = 0.0\n",
    "        for ngram in ngrams:\n",
    "            prob = self.get_probability(ngram)\n",
    "            if prob > 0:\n",
    "                log_prob += math.log(prob)\n",
    "            else:\n",
    "                log_prob += math.log(1e-10)  # 平滑處理\n",
    "        \n",
    "        return log_prob\n",
    "    \n",
    "    def calculate_perplexity(self, test_file: str) -> float:\n",
    "        \"\"\"\n",
    "        計算測試集的困惑度 (perplexity)\n",
    "        \n",
    "        Args:\n",
    "            test_file (str): 測試文件路徑\n",
    "            \n",
    "        Returns:\n",
    "            float: 困惑度值\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Calculating {self.n}-gram model perplexity...\")\n",
    "        \n",
    "        total_log_prob = 0.0\n",
    "        total_words = 0\n",
    "        line_count = 0\n",
    "        \n",
    "        with open(test_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                words = self.preprocess_text(line)\n",
    "                if len(words) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # 計算句子概率\n",
    "                log_prob = self.get_sentence_probability(line)\n",
    "                total_log_prob += log_prob\n",
    "                total_words += len(words)\n",
    "                \n",
    "        \n",
    "        # 計算困惑度: PP = exp(-1/N * sum(log P(sentence)))\n",
    "        avg_log_prob = total_log_prob / total_words\n",
    "        perplexity = math.exp(-avg_log_prob)\n",
    "        \n",
    "        self.logger.info(f\"測試集總詞數: {total_words}\")\n",
    "        self.logger.info(f\"平均對數概率: {avg_log_prob:.6f}\")\n",
    "        self.logger.info(f\"{self.n}-gram 困惑度: {perplexity:.2f}\")\n",
    "        \n",
    "        return perplexity\n",
    "    \n",
    "    def generate_text(self, context: Tuple[str, ...], max_length: int = 20) -> str:\n",
    "        \"\"\"\n",
    "        基於給定context生成文本\n",
    "        \n",
    "        Args:\n",
    "            context (Tuple[str, ...]): 初始context\n",
    "            max_length (int): 最大生成長度\n",
    "            \n",
    "        Returns:\n",
    "            str: 生成的文本\n",
    "        \"\"\"\n",
    "        if len(context) != self.n - 1:\n",
    "            raise ValueError(f\"Context長度應為 {self.n - 1}\")\n",
    "        \n",
    "        result = list(context)\n",
    "        current_context = context\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # 找到所有以current_context開頭的n-gram\n",
    "            candidates = []\n",
    "            for ngram, count in self.ngram_counts.items():\n",
    "                if ngram[:-1] == current_context:\n",
    "                    candidates.extend([ngram[-1]] * count)\n",
    "            \n",
    "            if not candidates:\n",
    "                break\n",
    "            \n",
    "            # 隨機選擇下一個詞\n",
    "            import random\n",
    "            next_word = random.choice(candidates)\n",
    "            \n",
    "            if next_word == self.end_token:\n",
    "                break\n",
    "            \n",
    "            result.append(next_word)\n",
    "            # 更新context\n",
    "            current_context = tuple(result[-(self.n-1):])\n",
    "        \n",
    "        # 移除開始標記\n",
    "        filtered_result = [word for word in result if word != self.start_token]\n",
    "        return ' '.join(filtered_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79697132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "N-gram語言模型訓練和測試程式\n",
    "使用 train.txt 訓練模型，在 test.txt 上評估性能\n",
    "比較 n=2 (bigram) 和 n=3 (trigram) 的表現\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import os\n",
    "from ngram_model import NGramModel\n",
    "\n",
    "def main():\n",
    "    \n",
    "    train_file = \"train.txt\"\n",
    "    test_file = \"test.txt\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"N-gram model Training and Evaluation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    trained_models = {}  # Store trained models for text generation\n",
    "    \n",
    "    for n in [2, 3]:\n",
    "        print(f\"\\n{'='*20} N={n} ({'Bigram' if n==2 else 'Trigram'}) {'='*20}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model = NGramModel(n=n)\n",
    "\n",
    "        print(f\"Train {n}-gram model...\")\n",
    "        model.train(train_file)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "        # Calculate perplexity\n",
    "        start_time = time.time()\n",
    "        perplexity = model.calculate_perplexity(test_file)\n",
    "        test_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Testing time: {test_time:.2f} seconds\")\n",
    "        \n",
    "        # save results and model\n",
    "        results[n] = {\n",
    "            'perplexity': perplexity,\n",
    "            'training_time': training_time,\n",
    "            'test_time': test_time,\n",
    "            'vocab_size': len(model.vocabulary),\n",
    "            'total_words': model.total_words,\n",
    "            'ngram_types': len(model.ngram_counts),\n",
    "            'context_types': len(model.context_counts)\n",
    "        }\n",
    "        trained_models[n] = model  # Store the trained model\n",
    "\n",
    "        print(f\"\\n{n}-gram model text generation examples:\")\n",
    "        try:\n",
    "            if n == 2:\n",
    "                contexts = [(\"add\",), (\"cook\",), (\"bake\",)]\n",
    "            else:\n",
    "                contexts = [(\"add\", \"the\"), (\"cook\", \"for\"), (\"bake\", \"at\")]\n",
    "            \n",
    "            for context in contexts:\n",
    "                generated = model.generate_text(context, max_length=15)\n",
    "                print(f\"  Context: {' '.join(context)} -> {generated}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error occurred during text generation: {e}\")\n",
    "        \n",
    "        # Test with incomplete.txt for text completion\n",
    "        if os.path.exists(\"incomplete.txt\"):\n",
    "            try:\n",
    "                with open(\"incomplete.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "                    incomplete_lines = [line.strip() for line in f if line.strip()]\n",
    "                \n",
    "                for i, incomplete_text in enumerate(incomplete_lines):\n",
    "                    words = model.preprocess_text(incomplete_text)\n",
    "                    if len(words) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Create context based on model type\n",
    "                    if n == 2:\n",
    "                            continue\n",
    "                    else:  # n == 3\n",
    "                        print(f\"\\n{n}-gram model incomplete text completion:\")\n",
    "                        if len(words) >= 2:\n",
    "                            context = (words[-2], words[-1])  # Use last 2 words as context\n",
    "                        elif len(words) == 1:\n",
    "                            context = (\"<s>\", words[-1])  # Pad with start token\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    # Generate completion\n",
    "                    completion = model.generate_text(context, max_length=20)\n",
    "                    # Remove the context words from completion to show only new words\n",
    "                    context_str = ' '.join(context)\n",
    "                    if completion.startswith(context_str):\n",
    "                        new_words = completion[len(context_str):].strip()\n",
    "                        if new_words:\n",
    "                            full_completion = incomplete_text + \" \" + new_words\n",
    "                        else:\n",
    "                            full_completion = incomplete_text + \" [no completion]\"\n",
    "                    else:\n",
    "                        full_completion = incomplete_text + \" \" + completion\n",
    "                    \n",
    "                    print(f\"  '{incomplete_text}' -> '{full_completion}'\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing incomplete.txt: {e}\")\n",
    "        else:\n",
    "            print(\"  incomplete.txt not found, skipping completion test\")\n",
    "\n",
    "    # Results comparison\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Results Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(f\"{'Metric':<20} {'Bigram (n=2)':<15} {'Trigram (n=3)':<15} {'Difference':<15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    bigram_pp = results[2]['perplexity']\n",
    "    trigram_pp = results[3]['perplexity']\n",
    "    pp_diff = ((trigram_pp - bigram_pp) / bigram_pp) * 100\n",
    "\n",
    "    print(f\"{'Perplexity':<20} {bigram_pp:<15.2f} {trigram_pp:<15.2f} {pp_diff:+.2f}%\")\n",
    "\n",
    "    bigram_time = results[2]['training_time']\n",
    "    trigram_time = results[3]['training_time']\n",
    "    time_diff = ((trigram_time - bigram_time) / bigram_time) * 100\n",
    "\n",
    "    print(f\"{'Training time':<20} {bigram_time:<15.2f} {trigram_time:<15.2f} {time_diff:+.2f}%\")\n",
    "    \n",
    "    bigram_ngrams = results[2]['ngram_types']  \n",
    "    trigram_ngrams = results[3]['ngram_types']\n",
    "    ngram_diff = ((trigram_ngrams - bigram_ngrams) / bigram_ngrams) * 100\n",
    "\n",
    "    print(f\"{'N-gram types':<20} {bigram_ngrams:<15,} {trigram_ngrams:<15,} {ngram_diff:+.2f}%\")\n",
    "\n",
    "    print(f\"{'Vocabulary size':<20} {results[2]['vocab_size']:<15,} {results[3]['vocab_size']:<15,} {'Same':<15}\")\n",
    "    print(f\"{'Total words':<20} {results[2]['total_words']:<15,} {results[3]['total_words']:<15,} {'Same':<15}\")\n",
    "\n",
    "    # Save detailed results to a file\n",
    "    with open(\"ngram_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"N-gram Language Model Evaluation Results\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        \n",
    "        for n in [2, 3]:\n",
    "            model_name = \"Bigram\" if n == 2 else \"Trigram\"\n",
    "            f.write(f\"{model_name} (n={n}) Results:\\n\")\n",
    "            f.write(f\"  Perplexity: {results[n]['perplexity']:.2f}\\n\")\n",
    "            f.write(f\"  Training time: {results[n]['training_time']:.2f} seconds\\n\")\n",
    "            f.write(f\"  Testing time: {results[n]['test_time']:.2f} seconds\\n\")\n",
    "            f.write(f\"  Vocabulary size: {results[n]['vocab_size']:,}\\n\")\n",
    "            f.write(f\"  Total words: {results[n]['total_words']:,}\\n\")\n",
    "            f.write(f\"  N-gram types: {results[n]['ngram_types']:,}\\n\")\n",
    "            f.write(f\"  Context types: {results[n]['context_types']:,}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        f.write(\"Comparison Results:\\n\")\n",
    "        f.write(f\"  Perplexity difference: {pp_diff:+.2f}%\\n\")\n",
    "        f.write(f\"  Training time difference: {time_diff:+.2f}%\\n\")\n",
    "        f.write(f\"  N-gram types difference: {ngram_diff:+.2f}%\\n\")\n",
    "        \n",
    "        # Save text generation examples using trained models\n",
    "        f.write(\"\\nText Generation Examples:\\n\")\n",
    "        for n in [2, 3]:\n",
    "            model_name = \"Bigram\" if n == 2 else \"Trigram\"\n",
    "            f.write(f\"\\n{model_name} Generation Examples:\\n\")\n",
    "            \n",
    "            model = trained_models[n]  # Use already trained model\n",
    "            \n",
    "            # Basic examples\n",
    "            if n == 2:\n",
    "                contexts = [(\"add\",), (\"cook\",), (\"bake\",)]\n",
    "            else:\n",
    "                contexts = [(\"add\", \"the\"), (\"cook\", \"for\"), (\"bake\", \"at\")]\n",
    "            \n",
    "            for context in contexts:\n",
    "                try:\n",
    "                    generated = model.generate_text(context, max_length=15)\n",
    "                    f.write(f\"  Context: {' '.join(context)} -> {generated}\\n\")\n",
    "                except Exception as e:\n",
    "                    f.write(f\"  Context: {' '.join(context)} -> Error: {e}\\n\")\n",
    "            \n",
    "            # Incomplete text completion examples\n",
    "            if os.path.exists(\"incomplete.txt\"):\n",
    "                f.write(f\"\\n{model_name} Incomplete Text Completions:\\n\")\n",
    "                try:\n",
    "                    with open(\"incomplete.txt\", \"r\", encoding=\"utf-8\") as inc_f:\n",
    "                        incomplete_lines = [line.strip() for line in inc_f if line.strip()]\n",
    "                    \n",
    "                    for incomplete_text in incomplete_lines[:8]:\n",
    "                        words = model.preprocess_text(incomplete_text)\n",
    "                        if len(words) == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        if n == 2:\n",
    "                            if len(words) >= 1:\n",
    "                                context = (words[-1],)\n",
    "                            else:\n",
    "                                continue\n",
    "                        else:\n",
    "                            if len(words) >= 2:\n",
    "                                context = (words[-2], words[-1])\n",
    "                            elif len(words) == 1:\n",
    "                                context = (\"<s>\", words[-1])\n",
    "                            else:\n",
    "                                continue\n",
    "                        \n",
    "                        try:\n",
    "                            completion = model.generate_text(context, max_length=8)\n",
    "                            context_str = ' '.join(context)\n",
    "                            if completion.startswith(context_str):\n",
    "                                new_words = completion[len(context_str):].strip()\n",
    "                                if new_words:\n",
    "                                    full_completion = incomplete_text + \" \" + new_words\n",
    "                                else:\n",
    "                                    full_completion = incomplete_text + \" [no completion]\"\n",
    "                            else:\n",
    "                                full_completion = incomplete_text + \" \" + completion\n",
    "                            f.write(f\"  '{incomplete_text}' -> '{full_completion}'\\n\")\n",
    "                        except Exception as e:\n",
    "                            f.write(f\"  '{incomplete_text}' -> Error: {e}\\n\")\n",
    "                except Exception as e:\n",
    "                    f.write(f\"  Error processing incomplete.txt: {e}\\n\")\n",
    "\n",
    "    print(f\"\\nDetailed results have been saved to ngram_results.txt\")\n",
    "    print(\"Program execution completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466d24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RNN Language Model Implementation\n",
    "使用 PyTorch 實現循環神經網路語言模型\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"詞彙表類別，處理詞彙到索引的轉換\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_count = Counter()\n",
    "        \n",
    "        # 特殊標記\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.start_token = '<START>'\n",
    "        self.end_token = '<END>'\n",
    "        \n",
    "        # 初始化特殊標記\n",
    "        self.add_word(self.pad_token)\n",
    "        self.add_word(self.unk_token)\n",
    "        self.add_word(self.start_token)\n",
    "        self.add_word(self.end_token)\n",
    "        \n",
    "    def add_word(self, word: str) -> int:\n",
    "        \"\"\"添加詞彙到詞彙表\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "        self.word_count[word] += 1\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 2):\n",
    "        \"\"\"建立詞彙表\"\"\"\n",
    "        logger.info(\"Building vocabulary...\")\n",
    "        \n",
    "        # 統計詞頻\n",
    "        for text in texts:\n",
    "            words = self.preprocess_text(text)\n",
    "            for word in words:\n",
    "                self.word_count[word] += 1\n",
    "        \n",
    "        # 添加高頻詞到詞彙表\n",
    "        for word, count in self.word_count.items():\n",
    "            if count >= min_freq and word not in self.word2idx:\n",
    "                self.add_word(word)\n",
    "        \n",
    "        logger.info(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "        logger.info(f\"Most common words: {self.word_count.most_common(10)}\")\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"文本預處理\"\"\"\n",
    "        text = text.lower().strip()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return words\n",
    "    \n",
    "    def text_to_indices(self, text: str) -> List[int]:\n",
    "        \"\"\"將文本轉換為索引序列\"\"\"\n",
    "        words = self.preprocess_text(text)\n",
    "        indices = [self.word2idx[self.start_token]]\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.word2idx:\n",
    "                indices.append(self.word2idx[word])\n",
    "            else:\n",
    "                indices.append(self.word2idx[self.unk_token])\n",
    "        \n",
    "        indices.append(self.word2idx[self.end_token])\n",
    "        return indices\n",
    "    \n",
    "    def indices_to_text(self, indices: List[int]) -> str:\n",
    "        \"\"\"將索引序列轉換為文本\"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            if idx in self.idx2word:\n",
    "                word = self.idx2word[idx]\n",
    "                if word not in [self.pad_token, self.start_token, self.end_token]:\n",
    "                    words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"文本數據集類別\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], vocab: Vocabulary, seq_length: int = 50):\n",
    "        self.vocab = vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.sequences = []\n",
    "        \n",
    "        self.prepare_sequences(texts)\n",
    "    \n",
    "    def prepare_sequences(self, texts: List[str]):\n",
    "        \"\"\"準備訓練序列\"\"\"\n",
    "        logger.info(\"Preparing training sequences...\")\n",
    "        \n",
    "        for text in texts:\n",
    "            indices = self.vocab.text_to_indices(text)\n",
    "            \n",
    "            # 如果序列太短，跳過\n",
    "            if len(indices) < 2:\n",
    "                continue\n",
    "            \n",
    "            # 創建滑動窗口序列\n",
    "            for i in range(len(indices) - 1):\n",
    "                # 輸入序列和目標序列\n",
    "                input_seq = indices[max(0, i - self.seq_length + 1):i + 1]\n",
    "                target = indices[i + 1]\n",
    "                \n",
    "                # 填充到固定長度\n",
    "                if len(input_seq) < self.seq_length:\n",
    "                    padding = [self.vocab.word2idx[self.vocab.pad_token]] * (self.seq_length - len(input_seq))\n",
    "                    input_seq = padding + input_seq\n",
    "                \n",
    "                self.sequences.append((input_seq, target))\n",
    "        \n",
    "        logger.info(f\"Created {len(self.sequences)} training sequences\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target = self.sequences[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    \"\"\"RNN 語言模型\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 128, \n",
    "                 num_layers: int = 2, dropout: float = 0.2):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 詞嵌入層\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # RNN 層\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, num_layers, \n",
    "                         batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Dropout 層\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 輸出層\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # 初始化權重\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化模型權重\"\"\"\n",
    "        init_range = 0.1\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-init_range, init_range)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"前向傳播\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 詞嵌入\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # RNN\n",
    "        rnn_out, hidden = self.rnn(embedded, hidden)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 取最後一個時間步的輸出\n",
    "        last_output = rnn_out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        output = self.dropout(last_output)\n",
    "        \n",
    "        # 線性層\n",
    "        output = self.linear(output)  # (batch_size, vocab_size)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"初始化隱藏狀態\"\"\"\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "\n",
    "class RNNTrainer:\n",
    "    \"\"\"RNN 訓練器\"\"\"\n",
    "    \n",
    "    def __init__(self, model, vocab, device):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        \n",
    "        # 損失函數和優化器\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "    def train_epoch(self, dataloader):\n",
    "        \"\"\"訓練一個 epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "            data, targets = data.to(self.device), targets.to(self.device)\n",
    "            batch_size = data.size(0)\n",
    "            \n",
    "            # 初始化隱藏狀態\n",
    "            hidden = self.model.init_hidden(batch_size, self.device)\n",
    "            \n",
    "            # 前向傳播\n",
    "            outputs, _ = self.model(data, hidden)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # 反向傳播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                logger.info(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        return total_loss / total_samples\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \"\"\"評估模型\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in dataloader:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                batch_size = data.size(0)\n",
    "                \n",
    "                hidden = self.model.init_hidden(batch_size, self.device)\n",
    "                outputs, _ = self.model(data, hidden)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                total_loss += loss.item() * batch_size\n",
    "                total_samples += batch_size\n",
    "        \n",
    "        return total_loss / total_samples\n",
    "    \n",
    "    def generate_text(self, start_text: str, max_length: int = 50, temperature: float = 1.0):\n",
    "        \"\"\"生成文本\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 預處理起始文本\n",
    "        indices = self.vocab.text_to_indices(start_text)\n",
    "        if len(indices) == 0:\n",
    "            indices = [self.vocab.word2idx[self.vocab.start_token]]\n",
    "        \n",
    "        generated = indices.copy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                # 準備輸入序列\n",
    "                input_seq = generated[-50:]  # 取最後50個詞作為上下文\n",
    "                if len(input_seq) < 50:\n",
    "                    padding = [self.vocab.word2idx[self.vocab.pad_token]] * (50 - len(input_seq))\n",
    "                    input_seq = padding + input_seq\n",
    "                \n",
    "                input_tensor = torch.tensor([input_seq], dtype=torch.long).to(self.device)\n",
    "                hidden = self.model.init_hidden(1, self.device)\n",
    "                \n",
    "                # 預測下一個詞\n",
    "                outputs, _ = self.model(input_tensor, hidden)\n",
    "                outputs = outputs / temperature\n",
    "                probabilities = torch.softmax(outputs, dim=-1)\n",
    "                \n",
    "                # 隨機採樣\n",
    "                next_word_idx = torch.multinomial(probabilities, 1).item()\n",
    "                \n",
    "                # 如果生成結束標記，停止生成\n",
    "                if next_word_idx == self.vocab.word2idx[self.vocab.end_token]:\n",
    "                    break\n",
    "                \n",
    "                generated.append(next_word_idx)\n",
    "        \n",
    "        return self.vocab.indices_to_text(generated)\n",
    "\n",
    "def load_data(file_path: str) -> List[str]:\n",
    "    \"\"\"載入訓練數據\"\"\"\n",
    "    texts = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    texts.append(line)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File {file_path} not found!\")\n",
    "        return []\n",
    "    \n",
    "    logger.info(f\"Loaded {len(texts)} texts from {file_path}\")\n",
    "    return texts\n",
    "\n",
    "def test_incomplete_sentences(trainer, incomplete_file: str):\n",
    "    \"\"\"測試不完整句子的補全\"\"\"\n",
    "    logger.info(\"Testing incomplete sentence completion...\")\n",
    "    \n",
    "    try:\n",
    "        with open(incomplete_file, 'r', encoding='utf-8') as f:\n",
    "            incomplete_texts = [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File {incomplete_file} not found!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RNN 模型文本補全結果\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for incomplete_text in incomplete_texts[:10]:  # 測試前10個\n",
    "        completed = trainer.generate_text(incomplete_text, max_length=20, temperature=0.8)\n",
    "        print(f\"輸入: {incomplete_text}\")\n",
    "        print(f\"補全: {completed}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    HIDDEN_DIM = 128\n",
    "    NUM_LAYERS = 2\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 10\n",
    "    BATCH_SIZE = 32\n",
    "    SEQ_LENGTH = 50\n",
    "    \n",
    "    # Load data\n",
    "    train_texts = load_data('train.txt')\n",
    "    if not train_texts:\n",
    "        return\n",
    "\n",
    "    # Build vocabulary\n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_vocab(train_texts, min_freq=3)\n",
    "\n",
    "    # Create dataset\n",
    "    train_dataset = TextDataset(train_texts, vocab, SEQ_LENGTH)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    model = RNNLanguageModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=HIDDEN_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS\n",
    "    ).to(device)\n",
    "    \n",
    "    logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # 創建訓練器\n",
    "    trainer = RNNTrainer(model, vocab, device)\n",
    "    \n",
    "    # 訓練模型\n",
    "    logger.info(\"Starting training...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = trainer.train_epoch(train_dataloader)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        logger.info(f'Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Time: {epoch_time:.2f}s')\n",
    "        \n",
    "        # 每幾個 epoch 生成一些文本\n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            print(f\"\\nEpoch {epoch+1} 生成範例:\")\n",
    "            sample_text = trainer.generate_text(\"add salt\", max_length=15)\n",
    "            print(f\"Generated: {sample_text}\")\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab': vocab,\n",
    "        'hyperparameters': {\n",
    "            'vocab_size': len(vocab),\n",
    "            'embed_dim': HIDDEN_DIM,\n",
    "            'hidden_dim': HIDDEN_DIM,\n",
    "            'num_layers': NUM_LAYERS\n",
    "        }\n",
    "    }, 'rnn_model.pth')\n",
    "    \n",
    "    logger.info(\"Model saved to rnn_model.pth\")\n",
    "    \n",
    "    # 測試不完整句子補全\n",
    "    test_incomplete_sentences(trainer, 'incomplete.txt')\n",
    "    \n",
    "    # 生成一些範例文本\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RNN 模型文本生成範例\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_prompts = [\"add\", \"cook\", \"bake\", \"mix\"]\n",
    "    for prompt in test_prompts:\n",
    "        generated = trainer.generate_text(prompt, max_length=20)\n",
    "        print(f\"起始詞: {prompt}\")\n",
    "        print(f\"生成文本: {generated}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d6980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "LSTM Language Model Implementation\n",
    "使用 PyTorch 實現長短期記憶網路語言模型\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"詞彙表類別，處理詞彙到索引的轉換\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_count = Counter()\n",
    "        \n",
    "        # 特殊標記\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.start_token = '<START>'\n",
    "        self.end_token = '<END>'\n",
    "        \n",
    "        # 初始化特殊標記\n",
    "        self.add_word(self.pad_token)\n",
    "        self.add_word(self.unk_token)\n",
    "        self.add_word(self.start_token)\n",
    "        self.add_word(self.end_token)\n",
    "        \n",
    "    def add_word(self, word: str) -> int:\n",
    "        \"\"\"添加詞彙到詞彙表\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "        self.word_count[word] += 1\n",
    "        return self.word2idx[word]\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 2):\n",
    "        \"\"\"建立詞彙表\"\"\"\n",
    "        logger.info(\"Building vocabulary...\")\n",
    "        \n",
    "        # 統計詞頻\n",
    "        for text in texts:\n",
    "            words = self.preprocess_text(text)\n",
    "            for word in words:\n",
    "                self.word_count[word] += 1\n",
    "        \n",
    "        # 添加高頻詞到詞彙表\n",
    "        for word, count in self.word_count.items():\n",
    "            if count >= min_freq and word not in self.word2idx:\n",
    "                self.add_word(word)\n",
    "        \n",
    "        logger.info(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "        logger.info(f\"Most common words: {self.word_count.most_common(10)}\")\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"文本預處理\"\"\"\n",
    "        text = text.lower().strip()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        return words\n",
    "    \n",
    "    def text_to_indices(self, text: str) -> List[int]:\n",
    "        \"\"\"將文本轉換為索引序列\"\"\"\n",
    "        words = self.preprocess_text(text)\n",
    "        indices = [self.word2idx[self.start_token]]\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.word2idx:\n",
    "                indices.append(self.word2idx[word])\n",
    "            else:\n",
    "                indices.append(self.word2idx[self.unk_token])\n",
    "        \n",
    "        indices.append(self.word2idx[self.end_token])\n",
    "        return indices\n",
    "    \n",
    "    def indices_to_text(self, indices: List[int]) -> str:\n",
    "        \"\"\"將索引序列轉換為文本\"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            if idx in self.idx2word:\n",
    "                word = self.idx2word[idx]\n",
    "                if word not in [self.pad_token, self.start_token, self.end_token]:\n",
    "                    words.append(word)\n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"文本數據集類別\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], vocab: Vocabulary, seq_length: int = 50):\n",
    "        self.vocab = vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.sequences = []\n",
    "        \n",
    "        self.prepare_sequences(texts)\n",
    "    \n",
    "    def prepare_sequences(self, texts: List[str]):\n",
    "        \"\"\"準備訓練序列\"\"\"\n",
    "        logger.info(\"Preparing training sequences...\")\n",
    "        \n",
    "        for text in texts:\n",
    "            indices = self.vocab.text_to_indices(text)\n",
    "            \n",
    "            # 如果序列太短，跳過\n",
    "            if len(indices) < 2:\n",
    "                continue\n",
    "            \n",
    "            # 創建滑動窗口序列\n",
    "            for i in range(len(indices) - 1):\n",
    "                # 輸入序列和目標序列\n",
    "                input_seq = indices[max(0, i - self.seq_length + 1):i + 1]\n",
    "                target = indices[i + 1]\n",
    "                \n",
    "                # 填充到固定長度\n",
    "                if len(input_seq) < self.seq_length:\n",
    "                    padding = [self.vocab.word2idx[self.vocab.pad_token]] * (self.seq_length - len(input_seq))\n",
    "                    input_seq = padding + input_seq\n",
    "                \n",
    "                self.sequences.append((input_seq, target))\n",
    "        \n",
    "        logger.info(f\"Created {len(self.sequences)} training sequences\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, target = self.sequences[idx]\n",
    "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    \"\"\"LSTM 語言模型\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 128, hidden_dim: int = 128, \n",
    "                 num_layers: int = 2, dropout: float = 0.2):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # 詞嵌入層\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # LSTM 層\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Dropout 層\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 輸出層\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # 初始化權重\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"初始化模型權重\"\"\"\n",
    "        init_range = 0.1\n",
    "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-init_range, init_range)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"前向傳播\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 詞嵌入\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # 取最後一個時間步的輸出\n",
    "        last_output = lstm_out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        output = self.dropout(last_output)\n",
    "        \n",
    "        # 線性層\n",
    "        output = self.linear(output)  # (batch_size, vocab_size)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"初始化隱藏狀態和細胞狀態\"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "class LSTMTrainer:\n",
    "    \"\"\"LSTM 訓練器\"\"\"\n",
    "    \n",
    "    def __init__(self, model, vocab, device):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        \n",
    "        # 損失函數和優化器\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[vocab.pad_token])\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "    def train_epoch(self, dataloader):\n",
    "        \"\"\"訓練一個 epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "            data, targets = data.to(self.device), targets.to(self.device)\n",
    "            batch_size = data.size(0)\n",
    "            \n",
    "            # 初始化隱藏狀態\n",
    "            hidden = self.model.init_hidden(batch_size, self.device)\n",
    "            \n",
    "            # 前向傳播\n",
    "            outputs, _ = self.model(data, hidden)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # 反向傳播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                logger.info(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        return total_loss / total_samples\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        \"\"\"評估模型\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in dataloader:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                batch_size = data.size(0)\n",
    "                \n",
    "                hidden = self.model.init_hidden(batch_size, self.device)\n",
    "                outputs, _ = self.model(data, hidden)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                total_loss += loss.item() * batch_size\n",
    "                total_samples += batch_size\n",
    "        \n",
    "        return total_loss / total_samples\n",
    "    \n",
    "    def generate_text(self, start_text: str, max_length: int = 50, temperature: float = 1.0):\n",
    "        \"\"\"生成文本\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # 預處理起始文本\n",
    "        indices = self.vocab.text_to_indices(start_text)\n",
    "        if len(indices) == 0:\n",
    "            indices = [self.vocab.word2idx[self.vocab.start_token]]\n",
    "        \n",
    "        generated = indices.copy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                # 準備輸入序列\n",
    "                input_seq = generated[-50:]  # 取最後50個詞作為上下文\n",
    "                if len(input_seq) < 50:\n",
    "                    padding = [self.vocab.word2idx[self.vocab.pad_token]] * (50 - len(input_seq))\n",
    "                    input_seq = padding + input_seq\n",
    "                \n",
    "                input_tensor = torch.tensor([input_seq], dtype=torch.long).to(self.device)\n",
    "                hidden = self.model.init_hidden(1, self.device)\n",
    "                \n",
    "                # 預測下一個詞\n",
    "                outputs, _ = self.model(input_tensor, hidden)\n",
    "                outputs = outputs / temperature\n",
    "                probabilities = torch.softmax(outputs, dim=-1)\n",
    "                \n",
    "                # 隨機採樣\n",
    "                next_word_idx = torch.multinomial(probabilities, 1).item()\n",
    "                \n",
    "                # 如果生成結束標記，停止生成\n",
    "                if next_word_idx == self.vocab.word2idx[self.vocab.end_token]:\n",
    "                    break\n",
    "                \n",
    "                generated.append(next_word_idx)\n",
    "        \n",
    "        return self.vocab.indices_to_text(generated)\n",
    "\n",
    "def load_data(file_path: str) -> List[str]:\n",
    "    \"\"\"載入訓練數據\"\"\"\n",
    "    texts = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    texts.append(line)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File {file_path} not found!\")\n",
    "        return []\n",
    "    \n",
    "    logger.info(f\"Loaded {len(texts)} texts from {file_path}\")\n",
    "    return texts\n",
    "\n",
    "def test_incomplete_sentences(trainer, incomplete_file: str):\n",
    "    \"\"\"測試不完整句子的補全\"\"\"\n",
    "    logger.info(\"Testing incomplete sentence completion...\")\n",
    "    \n",
    "    try:\n",
    "        with open(incomplete_file, 'r', encoding='utf-8') as f:\n",
    "            incomplete_texts = [line.strip() for line in f if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File {incomplete_file} not found!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LSTM 模型文本補全結果\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for incomplete_text in incomplete_texts[:10]:  # 測試前10個\n",
    "        completed = trainer.generate_text(incomplete_text, max_length=20, temperature=0.8)\n",
    "        print(f\"輸入: {incomplete_text}\")\n",
    "        print(f\"補全: {completed}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函數\"\"\"\n",
    "    # 設置設備\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # 超參數\n",
    "    HIDDEN_DIM = 128\n",
    "    NUM_LAYERS = 2\n",
    "    LEARNING_RATE = 0.001\n",
    "    NUM_EPOCHS = 10\n",
    "    BATCH_SIZE = 32\n",
    "    SEQ_LENGTH = 50\n",
    "    \n",
    "    # 載入數據\n",
    "    train_texts = load_data('train.txt')\n",
    "    if not train_texts:\n",
    "        return\n",
    "    \n",
    "    # 建立詞彙表\n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_vocab(train_texts, min_freq=3)\n",
    "    \n",
    "    # 創建數據集\n",
    "    train_dataset = TextDataset(train_texts, vocab, SEQ_LENGTH)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # 創建模型\n",
    "    model = LSTMLanguageModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=HIDDEN_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS\n",
    "    ).to(device)\n",
    "    \n",
    "    logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # 創建訓練器\n",
    "    trainer = LSTMTrainer(model, vocab, device)\n",
    "    \n",
    "    # 訓練模型\n",
    "    logger.info(\"Starting training...\")\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = trainer.train_epoch(train_dataloader)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        logger.info(f'Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Time: {epoch_time:.2f}s')\n",
    "        \n",
    "        # 每幾個 epoch 生成一些文本\n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            print(f\"\\nEpoch {epoch+1} 生成範例:\")\n",
    "            sample_text = trainer.generate_text(\"add salt\", max_length=15)\n",
    "            print(f\"Generated: {sample_text}\")\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab': vocab,\n",
    "        'hyperparameters': {\n",
    "            'vocab_size': len(vocab),\n",
    "            'embed_dim': HIDDEN_DIM,\n",
    "            'hidden_dim': HIDDEN_DIM,\n",
    "            'num_layers': NUM_LAYERS\n",
    "        }\n",
    "    }, 'lstm_model.pth')\n",
    "    \n",
    "    logger.info(\"Model saved to lstm_model.pth\")\n",
    "    \n",
    "    # 測試不完整句子補全\n",
    "    test_incomplete_sentences(trainer, 'incomplete.txt')\n",
    "    \n",
    "    # 生成一些範例文本\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LSTM 模型文本生成範例\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_prompts = [\"add\", \"cook\", \"bake\", \"mix\"]\n",
    "    for prompt in test_prompts:\n",
    "        generated = trainer.generate_text(prompt, max_length=20)\n",
    "        print(f\"起始詞: {prompt}\")\n",
    "        print(f\"生成文本: {generated}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
