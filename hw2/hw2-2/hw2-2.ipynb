{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c79a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# 忽略 FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B\"\n",
    "TRAIN_PATH = \"commonsense_15k.json\"\n",
    "OUTPUT_ROOT = \"./llama32_1b_team_lora\"  # 每個 setting 存不同子資料夾\n",
    "\n",
    "\n",
    "def extract_options_from_instruction(instruction: str, answer_format: str) -> List[str]:\n",
    "    if \"true/false\" in answer_format.lower():\n",
    "        return [\"true\", \"false\"]\n",
    "    \n",
    "    formats = {\n",
    "        \"Answer\": r\"Answer\\d+:\",\n",
    "        \"Solution\": r\"Solution\\d+:\",\n",
    "        \"Ending\": r\"Ending\\d+:\",\n",
    "        \"Option\": r\"Option\\d+:\",\n",
    "    }\n",
    "\n",
    "    detected = None\n",
    "    for fmt, pattern in formats.items():\n",
    "        if re.search(pattern, instruction):\n",
    "            detected = fmt\n",
    "            break\n",
    "\n",
    "    if detected is None:\n",
    "        return None\n",
    "\n",
    "    boundary = (\n",
    "        rf\"(?=\\s*{detected}\\d+:|\\s*Answer format:|\\s*Solution format:|$)\"\n",
    "    )\n",
    "\n",
    "    if detected in [\"Answer\", \"Option\"]:\n",
    "        # Multi-line safe version (handles same-line concatenated Answers)\n",
    "        pattern = rf\"({detected}\\d+:\\s*.*?){boundary}\"\n",
    "        matches = re.findall(pattern, instruction, re.DOTALL)\n",
    "        return [m.strip() for m in matches]\n",
    "\n",
    "    elif detected in [\"Solution\", \"Ending\"]:\n",
    "        # Multi-line blocks (may contain many sentences)\n",
    "        pattern = rf\"({detected}\\d+:\\s*.*?){boundary}\"\n",
    "        matches = re.findall(pattern, instruction, re.DOTALL)\n",
    "        return [m.strip() for m in matches]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_answer_key(answer: str) -> str:\n",
    "    \"\"\"\n",
    "    解析答案，返回標準化的答案\n",
    "    \"\"\"\n",
    "    answer = answer.lower().strip()\n",
    "    \n",
    "    # 直接的 true/false\n",
    "    if answer in [\"true\", \"false\"]:\n",
    "        return answer\n",
    "    \n",
    "    # solution1/solution2 格式\n",
    "    solution_match = re.search(r\"solution(\\d+)\", answer)\n",
    "    if solution_match:\n",
    "        return f\"solution{solution_match.group(1)}\"\n",
    "    \n",
    "    # answer1/answer2/answer3 格式\n",
    "    answer_match = re.search(r\"answer(\\d+)\", answer)\n",
    "    if answer_match:\n",
    "        return f\"answer{answer_match.group(1)}\"\n",
    "    \n",
    "    # ending1/ending2/ending3/ending4 格式\n",
    "    ending_match = re.search(r\"ending(\\d+)\", answer)\n",
    "    if ending_match:\n",
    "        return f\"ending{ending_match.group(1)}\"\n",
    "    \n",
    "    # option1/option2 格式\n",
    "    option_match = re.search(r\"option(\\d+)\", answer)\n",
    "    if option_match:\n",
    "        return f\"option{option_match.group(1)}\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "def get_correct_option_index(parsed_answer: str, options: List[str]) -> int:\n",
    "    \"\"\"\n",
    "    根據解析後的答案找到正確選項的索引\n",
    "    \"\"\"\n",
    "    if parsed_answer in [\"true\", \"false\"]:\n",
    "        try:\n",
    "            return options.index(parsed_answer)\n",
    "        except ValueError:\n",
    "            return -1\n",
    "    \n",
    "    # 對於其他格式，提取數字\n",
    "    number_match = re.search(r\"(\\d+)\", parsed_answer)\n",
    "    if number_match:\n",
    "        # 將1-based索引轉換為0-based索引\n",
    "        return int(number_match.group(1)) - 1\n",
    "    \n",
    "    return -1\n",
    "\n",
    "\n",
    "class TeamBinaryDataset(Dataset):\n",
    "    def __init__(self, path: str, tokenizer, max_length: int = 512):\n",
    "        self.samples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            for item in data:\n",
    "                question = self._extract_question(item[\"instruction\"])\n",
    "                answer_format = self._extract_answer_format(item[\"instruction\"])\n",
    "                \n",
    "                options = extract_options_from_instruction(item[\"instruction\"], answer_format)\n",
    "                if not options:\n",
    "                    continue\n",
    "                \n",
    "                parsed_answer = parse_answer_key(item[\"answer\"])\n",
    "                correct_idx = get_correct_option_index(parsed_answer, options)\n",
    "                \n",
    "                if correct_idx == -1:\n",
    "                    continue\n",
    "                \n",
    "                for i, option in enumerate(options):\n",
    "                    text = self.build_text(question, option)\n",
    "                    label = 1 if i == correct_idx else 0\n",
    "                    self.samples.append({\"text\": text, \"label\": label})\n",
    "\n",
    "    def _extract_question(self, instruction: str) -> str:\n",
    "        if \"question:\" in instruction.lower():\n",
    "            question_start = instruction.lower().find(\"question:\")\n",
    "            question_part = instruction[question_start + 9:]\n",
    "            \n",
    "            if \"answer format:\" in question_part.lower():\n",
    "                question = question_part.split(\"Answer format:\")[0].strip()\n",
    "            else:\n",
    "                question = question_part.strip()\n",
    "            return question\n",
    "        \n",
    "        lines = instruction.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.strip() and not any(keyword in line.lower() for keyword in \n",
    "                                      ['solution', 'answer', 'ending', 'option']):\n",
    "                return line.strip()\n",
    "        \n",
    "        return instruction.split('\\n')[0].strip() if instruction else \"\"\n",
    "\n",
    "    def _extract_answer_format(self, instruction: str) -> str:\n",
    "        format_match = re.search(r\"Answer format:\\s*([^\\n]+)\", instruction, re.IGNORECASE)\n",
    "        if format_match:\n",
    "            return format_match.group(1).strip()\n",
    "        return \"\"\n",
    "\n",
    "    def build_text(self, question: str, candidate_answer: str) -> str:\n",
    "        if len(candidate_answer) > 200:\n",
    "            candidate_answer = candidate_answer[:200] + \"...\"\n",
    "        \n",
    "        return f\"Question: {question}\\nCandidate answer: {candidate_answer}\\n\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        encoded[\"labels\"] = item[\"label\"]\n",
    "        return encoded\n",
    "\n",
    "# ================== LoRA 設定 ==================\n",
    "LORA_SETTINGS = {\n",
    "    \"attn_light\": {\n",
    "        \"description\": 'ATTN—light: [\"q_proj\", \"v_proj\"]',\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "    },\n",
    "    \"attn_ffn_medium\": {\n",
    "        \"description\": 'ATTN+FFN—medium: [\"q_proj\",\"k_proj\",\"v_proj\",\"up_proj\",\"down_proj\"]',\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\"],\n",
    "    },\n",
    "    \"full_heavy\": {\n",
    "        \"description\": 'Full—heavy: [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"up_proj\",\"down_proj\",\"gate_proj\"]',\n",
    "        \"target_modules\": [\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "            \"gate_proj\",\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def make_lora_model(base_model, target_modules: List[str]):\n",
    "    \"\"\"\n",
    "    把 base_model 包成 LoRA 版本，target_modules 依據 setting 不同\n",
    "    \"\"\"\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=target_modules,\n",
    "        task_type=\"SEQ_CLS\",  # sequence classification\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "\n",
    "# ================== 訓練 + 記錄 Learning Curve ==================\n",
    "def train_one_setting(\n",
    "    setting_name: str,\n",
    "    target_modules: List[str],\n",
    "    dataset: TeamBinaryDataset,\n",
    "    tokenizer,\n",
    "    num_epochs: int = 3,\n",
    "):\n",
    "    print(f\"\\n========== Training setting: {setting_name} ==========\")\n",
    "    print(f\"Target modules: {target_modules}\")\n",
    "\n",
    "    # 1. split train/val\n",
    "    val_ratio = 0.1\n",
    "    val_size = int(len(dataset) * val_ratio)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # 2. base model + LoRA\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\",\n",
    "    )\n",
    "    model = make_lora_model(base_model, target_modules)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    out_dir = os.path.join(OUTPUT_ROOT, setting_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.03,\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=[],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(out_dir)\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "    log_history = trainer.state.log_history\n",
    "\n",
    "    train_losses = []   # (epoch, loss)\n",
    "    eval_losses = []    # (epoch, loss)\n",
    "\n",
    "    for log in log_history:\n",
    "        if \"loss\" in log and \"epoch\" in log and \"eval_loss\" not in log:\n",
    "            train_losses.append((log[\"epoch\"], log[\"loss\"]))\n",
    "        if \"eval_loss\" in log and \"epoch\" in log:\n",
    "            eval_losses.append((log[\"epoch\"], log[\"eval_loss\"]))\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"eval_losses\": eval_losses,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58562cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "full_dataset = TeamBinaryDataset(TRAIN_PATH, tokenizer, max_length=512)\n",
    "\n",
    "all_curves = {}  # setting_name -> dict(train_losses, eval_losses)\n",
    "\n",
    "for setting_name, cfg in LORA_SETTINGS.items():\n",
    "    curves = train_one_setting(\n",
    "        setting_name=setting_name,\n",
    "        target_modules=cfg[\"target_modules\"],\n",
    "        dataset=full_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        num_epochs=3,\n",
    "    )\n",
    "    all_curves[setting_name] = curves\n",
    "\n",
    "# ================== 畫 Learning Curves ==================\n",
    "# 把三個設定畫在同一張圖（train 與 val 分開 subplot）\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# subplot 1: train loss\n",
    "plt.subplot(1, 2, 1)\n",
    "for setting_name, curves in all_curves.items():\n",
    "    epochs, losses = zip(*curves[\"train_losses\"])\n",
    "    plt.plot(epochs, losses, marker=\"o\", label=f\"{setting_name} (train)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss per Setting\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# subplot 2: eval loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for setting_name, curves in all_curves.items():\n",
    "    if len(curves[\"eval_losses\"]) == 0:\n",
    "        continue\n",
    "    epochs, losses = zip(*curves[\"eval_losses\"])\n",
    "    plt.plot(epochs, losses, marker=\"o\", label=f\"{setting_name} (val)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss per Setting\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs(OUTPUT_ROOT, exist_ok=True)\n",
    "plt.savefig(os.path.join(OUTPUT_ROOT, \"learning_curves.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb18ac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_data: pd.DataFrame, tokenizer, max_length: int = 512):\n",
    "        self.samples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.test_ids = []\n",
    "        \n",
    "        for idx, row in test_data.iterrows():\n",
    "            test_id = row['id']\n",
    "            instruction = row['instruction']\n",
    "            \n",
    "            # 提取問題和選項\n",
    "            question = self._extract_question(instruction)\n",
    "            answer_format = self._extract_answer_format(instruction)\n",
    "            options = extract_options_from_instruction(instruction, answer_format)\n",
    "            \n",
    "            if not options:\n",
    "                continue\n",
    "                \n",
    "            # 為每個選項創建一個樣本\n",
    "            for i, option in enumerate(options):\n",
    "                text = self.build_text(question, option)\n",
    "                self.samples.append({\n",
    "                    \"text\": text,\n",
    "                    \"test_id\": test_id,\n",
    "                    \"option_idx\": i,\n",
    "                    \"option_text\": option\n",
    "                })\n",
    "                self.test_ids.append(test_id)\n",
    "\n",
    "    def _extract_question(self, instruction: str) -> str:\n",
    "        if \"question:\" in instruction.lower():\n",
    "            question_start = instruction.lower().find(\"question:\")\n",
    "            question_part = instruction[question_start + 9:]\n",
    "            \n",
    "            # 找到第一個 Answer 的位置\n",
    "            answer_start = float('inf')\n",
    "            for pattern in [\"Answer1:\", \"Solution1:\", \"Ending1:\", \"Option1:\"]:\n",
    "                pos = question_part.find(pattern)\n",
    "                if pos != -1 and pos < answer_start:\n",
    "                    answer_start = pos\n",
    "            \n",
    "            if answer_start != float('inf'):\n",
    "                question = question_part[:answer_start].strip()\n",
    "            else:\n",
    "                question = question_part.strip()\n",
    "            return question\n",
    "        \n",
    "        # 如果沒有 \"question:\"，取第一行作為問題\n",
    "        lines = instruction.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.strip() and not any(keyword in line for keyword in \n",
    "                                      ['Answer1:', 'Solution1:', 'Ending1:', 'Option1:']):\n",
    "                return line.strip()\n",
    "        \n",
    "        return instruction.split('\\n')[0].strip() if instruction else \"\"\n",
    "\n",
    "    def _extract_answer_format(self, instruction: str) -> str:\n",
    "        # 根據內容判斷答案格式\n",
    "        if \"Answer1:\" in instruction:\n",
    "            return \"answer1/answer2/answer3/answer4\"\n",
    "        elif \"Solution1:\" in instruction:\n",
    "            return \"solution1/solution2\"\n",
    "        elif \"Ending1:\" in instruction:\n",
    "            return \"ending1/ending2/ending3/ending4\"\n",
    "        elif \"Option1:\" in instruction:\n",
    "            return \"option1/option2\"\n",
    "        return \"\"\n",
    "\n",
    "    def build_text(self, question: str, candidate_answer: str) -> str:\n",
    "        if len(candidate_answer) > 200:\n",
    "            candidate_answer = candidate_answer[:200] + \"...\"\n",
    "        \n",
    "        return f\"Question: {question}\\nCandidate answer: {candidate_answer}\\n\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "        # ✅ 修正：只返回 tokenizer 編碼的結果\n",
    "        encoded = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        \n",
    "        # ✅ 添加額外信息，但不在編碼結果中\n",
    "        return {\n",
    "            **encoded,  # 包含 input_ids, attention_mask\n",
    "            \"test_id\": item[\"test_id\"],\n",
    "            \"option_idx\": item[\"option_idx\"],\n",
    "            \"option_text\": item[\"option_text\"]\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    自定義的 collate 函數，處理批次數據\n",
    "    \"\"\"\n",
    "    # 分離數值數據和額外信息\n",
    "    tokenizer_keys = ['input_ids', 'attention_mask']\n",
    "    extra_keys = ['test_id', 'option_idx', 'option_text']\n",
    "    \n",
    "    # 處理 tokenizer 輸出\n",
    "    tokenizer_batch = {}\n",
    "    for key in tokenizer_keys:\n",
    "        if key in batch[0]:\n",
    "            tokenizer_batch[key] = [item[key] for item in batch]\n",
    "    \n",
    "    # 使用 DataCollatorWithPadding 處理\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    collated = data_collator(tokenizer_batch)\n",
    "    \n",
    "    # 添加額外信息\n",
    "    for key in extra_keys:\n",
    "        if key in batch[0]:\n",
    "            collated[key] = [item[key] for item in batch]\n",
    "    \n",
    "    return collated\n",
    "\n",
    "def predict_with_lora_model(model_path: str, test_csv_path: str, output_csv_path: str, tokenizer):\n",
    "    \"\"\"\n",
    "    使用訓練好的 LoRA 模型進行預測\n",
    "    \n",
    "    Args:\n",
    "        model_path: 訓練好的 LoRA 模型路徑\n",
    "        test_csv_path: 測試數據 CSV 路徑\n",
    "        output_csv_path: 輸出預測結果 CSV 路徑\n",
    "        tokenizer: 分詞器\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # 1. 載入測試數據\n",
    "    test_data = pd.read_csv(test_csv_path)\n",
    "    print(f\"載入 {len(test_data)} 條測試數據\")\n",
    "    \n",
    "    # 2. 創建測試數據集\n",
    "    test_dataset = TestDataset(test_data, tokenizer, max_length=512)\n",
    "    \n",
    "    # ✅ 使用自定義的 collate 函數\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=8, \n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    # 3. 載入模型並確保 pad_token_id 設置正確\n",
    "    from peft import PeftModel\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        num_labels=2,\n",
    "        problem_type=\"single_label_classification\",\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    \n",
    "    # ✅ 確保模型配置正確\n",
    "    if hasattr(base_model, 'config'):\n",
    "        base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # 4. 進行預測\n",
    "    predictions = {}  # test_id -> {option_idx: probability}\n",
    "    \n",
    "    print(\"開始預測...\")\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"預測中\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            test_ids = batch[\"test_id\"]\n",
    "            option_indices = batch[\"option_idx\"]\n",
    "            \n",
    "            # 前向傳播\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = torch.softmax(outputs.logits, dim=1)[:, 1]  # 取正確答案的機率\n",
    "            \n",
    "            # 收集預測結果\n",
    "            for test_id, option_idx, prob in zip(test_ids, option_indices, probabilities):\n",
    "                if test_id not in predictions:\n",
    "                    predictions[test_id] = {}\n",
    "                predictions[test_id][option_idx] = prob.item()\n",
    "    \n",
    "    # 5. 生成最終預測\n",
    "    results = []\n",
    "    for test_id, options in predictions.items():\n",
    "        # 找到機率最高的選項\n",
    "        best_option_idx = max(options.keys(), key=lambda k: options[k])\n",
    "        \n",
    "        # 根據選項索引生成答案標籤\n",
    "        test_row = test_data[test_data['id'] == test_id]['instruction'].iloc[0]\n",
    "        if \"Answer\" in test_row:\n",
    "            answer_label = f\"answer{best_option_idx + 1}\"\n",
    "        elif \"Solution\" in test_row:\n",
    "            answer_label = f\"solution{best_option_idx + 1}\"\n",
    "        elif \"Ending\" in test_row:\n",
    "            answer_label = f\"ending{best_option_idx + 1}\"\n",
    "        elif \"Option\" in test_row:\n",
    "            answer_label = f\"option{best_option_idx + 1}\"\n",
    "        else:\n",
    "            answer_label = f\"answer{best_option_idx + 1}\"  # 默認\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": test_id,\n",
    "            \"answer\": answer_label\n",
    "        })\n",
    "    \n",
    "    # 6. 保存結果\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"預測結果已保存到: {output_csv_path}\")\n",
    "    print(f\"預測了 {len(results)} 條數據\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def run_inference_for_all_settings(test_csv_path: str, output_dir: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for setting_name in LORA_SETTINGS.keys():\n",
    "        print(f\"\\n========== 使用 {setting_name} 模型進行預測 ==========\")\n",
    "        \n",
    "        model_path = os.path.join(OUTPUT_ROOT, setting_name)\n",
    "        output_csv = os.path.join(output_dir, f\"predictions_{setting_name}.csv\")\n",
    "        \n",
    "        try:\n",
    "            predict_with_lora_model(\n",
    "                model_path=model_path,\n",
    "                test_csv_path=test_csv_path,\n",
    "                output_csv_path=output_csv,\n",
    "                tokenizer=tokenizer\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"預測 {setting_name} 時發生錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv_path = \"test.csv\"  \n",
    "output_dir = \"./predictions\"\n",
    "\n",
    "run_inference_for_all_settings(test_csv_path, output_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "predict_with_lora_model(\n",
    "    model_path=\"./llama32_1b_team_lora/attn_light\",\n",
    "    test_csv_path=test_csv_path,\n",
    "    output_csv_path=\"final_predictions.csv\",\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
