{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-30T06:03:04.354017Z",
     "iopub.status.busy": "2025-10-30T06:03:04.353395Z",
     "iopub.status.idle": "2025-10-30T06:03:05.728785Z",
     "shell.execute_reply": "2025-10-30T06:03:05.728100Z",
     "shell.execute_reply.started": "2025-10-30T06:03:04.353992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/coconut/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:03:07.412049Z",
     "iopub.status.busy": "2025-10-30T06:03:07.411378Z",
     "iopub.status.idle": "2025-10-30T06:03:09.414534Z",
     "shell.execute_reply": "2025-10-30T06:03:09.413624Z",
     "shell.execute_reply.started": "2025-10-30T06:03:07.412024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# -------- Tokenizer --------\n",
    "def nltk_tokenizer(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "\n",
    "# -------- Read JSON (for DBPedia) --------\n",
    "def load_dbpedia_data(train_path, val_path, test_path=None):\n",
    "    # ---- train ----\n",
    "    with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        train_data = json.load(f)\n",
    "    train_texts = [item[\"text\"] for item in train_data]\n",
    "    train_labels = [int(item[\"label\"]) for item in train_data]\n",
    "\n",
    "    # ---- val ----\n",
    "    with open(val_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        val_data = json.load(f)\n",
    "    val_texts = [item[\"text\"] for item in val_data]\n",
    "    val_labels = [int(item[\"label\"]) for item in val_data]\n",
    "\n",
    "    # ---- test ----\n",
    "    if test_path is not None:\n",
    "        with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "        test_texts = [item[\"text\"] for item in test_data]\n",
    "        test_ids = [item.get(\"id\", i) for i, item in enumerate(test_data)]\n",
    "    else:\n",
    "        test_texts, test_ids = [], []\n",
    "\n",
    "    num_classes = max(train_labels + val_labels) + 1\n",
    "\n",
    "    return train_texts, train_labels, val_texts, val_labels, test_texts, test_ids, num_classes\n",
    "\n",
    "\n",
    "# -------- Vocab --------\n",
    "class Vocab:\n",
    "    def __init__(self, tokens_list, min_freq=1):\n",
    "        counter = Counter()\n",
    "        for tokens in tokens_list:\n",
    "            counter.update(tokens)\n",
    "\n",
    "        self.itos = [\"<unk>\", \"<pad>\"] + [tok for tok, freq in counter.items() if freq >= min_freq]\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, self.stoi[\"<unk>\"])\n",
    "\n",
    "\n",
    "# -------- Dataset --------\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, vocab=None, max_len=128):\n",
    "        self.tokens_list = [nltk_tokenizer(text) for text in tqdm(texts, desc=\"Tokenizing\")]\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "        if vocab is None:\n",
    "            self.vocab = Vocab(self.tokens_list)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens_list[idx]\n",
    "        ids = [self.vocab[tok] for tok in tokens[:self.max_len]]\n",
    "\n",
    "        if len(ids) < self.max_len:\n",
    "            ids += [self.vocab[\"<pad>\"]] * (self.max_len - len(ids))\n",
    "\n",
    "        ids = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
    "            return ids, label\n",
    "        else:\n",
    "            return ids\n",
    "\n",
    "\n",
    "# -------- DataLoader --------\n",
    "def get_dataloaders(train_texts, train_labels, val_texts, val_labels,\n",
    "                    batch_size=32, max_len=128):\n",
    "    train_dataset = TextDataset(train_texts, train_labels, max_len=max_len)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, vocab=train_dataset.vocab, max_len=max_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, train_dataset.vocab\n",
    "\n",
    "\n",
    "# -------- Test Loader --------\n",
    "def get_test_loader(test_texts, vocab, batch_size=32, max_len=128):\n",
    "    test_dataset = TextDataset(test_texts, labels=None, vocab=vocab, max_len=max_len)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic SSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:03:10.810789Z",
     "iopub.status.busy": "2025-10-30T06:03:10.809986Z",
     "iopub.status.idle": "2025-10-30T06:03:10.825721Z",
     "shell.execute_reply": "2025-10-30T06:03:10.824868Z",
     "shell.execute_reply.started": "2025-10-30T06:03:10.810762Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- basic SSM ---\n",
    "class DiagonalSSM(nn.Module):\n",
    "    def __init__(self, state_size: int, dt_init: float = 1.0, init_scale: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.state_size = state_size\n",
    "\n",
    "        # --- Continuous-time parameters ---\n",
    "        # A: diagonal, so is one-dimensional, negative for stability\n",
    "        self.A = nn.Parameter(-torch.rand(state_size) * init_scale)\n",
    "\n",
    "        # B, C: learnable projections\n",
    "        self.B = nn.Parameter(torch.randn(state_size) * init_scale)\n",
    "        self.C = nn.Parameter(torch.randn(state_size) * init_scale)\n",
    "\n",
    "        # Discretization step (Δt)\n",
    "        self.dt = nn.Parameter(torch.ones(1) * dt_init)\n",
    "\n",
    "        # Buffers for discretized\n",
    "        self.register_buffer(\"A_bar\", torch.zeros(state_size))\n",
    "        self.register_buffer(\"B_bar\", torch.zeros(state_size))\n",
    "\n",
    "        self._discretize()\n",
    "\n",
    "    def _discretize(self):\n",
    "        \"\"\"\n",
    "        Compute discrete-time:\n",
    "            A_bar = exp(A * dt)\n",
    "            B_bar = (exp(A * dt) - 1) / A * B\n",
    "        (diagonal version of zero-order hold discretization)\n",
    "        \"\"\"\n",
    "        A_dt = self.A * self.dt  # (state_size)\n",
    "        self.A_bar = torch.exp(A_dt)\n",
    "\n",
    "        # Handle division by zero for A ≈ 0\n",
    "        self.B_bar = torch.where(\n",
    "            self.A.abs() > 1e-5,\n",
    "            (self.A_bar - 1.0) / self.A * self.B,\n",
    "            self.dt * self.B,\n",
    "        )\n",
    "\n",
    "    def forward(self, u):\n",
    "        \"\"\"\n",
    "        u: (B, L, 1)\n",
    "        return: (B, L, state_size)\n",
    "        \"\"\"\n",
    "        B, L, _ = u.shape\n",
    "        self._discretize()  \n",
    "\n",
    "        # State h: (B, state_size)\n",
    "        h = torch.zeros(B, self.state_size, device=u.device)\n",
    "        outputs = []\n",
    "\n",
    "        # scan over sequence\n",
    "        for t in range(L):\n",
    "            u_t = u[:, t, 0]                     \n",
    "            h = self.A_bar * h + self.B_bar * u_t.unsqueeze(-1)\n",
    "            y_t = h * self.C                    \n",
    "            outputs.append(y_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "# --- Block ---\n",
    "class SSMBlock(nn.Module):\n",
    "    def __init__(self, in_dim: int, state_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Linear(in_dim, 1)\n",
    "        self.ssm = DiagonalSSM(state_size)\n",
    "        self.proj_out = nn.Linear(state_size, in_dim)\n",
    "        self.gate = nn.Sigmoid()\n",
    "        self.norm = nn.LayerNorm(in_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        u = self.proj_in(x)\n",
    "        y = self.ssm(u)\n",
    "        y = self.proj_out(y)\n",
    "        y = self.gate(y) * y\n",
    "        y = self.dropout(y)\n",
    "        return self.norm(residual + y)\n",
    "\n",
    "\n",
    "# --- Text Classifier ---\n",
    "class SSMTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_classes: int, state_size: int, num_layers: int,\n",
    "             emb_dim: int = 128, dropout: float = 0.1, pad_idx: int = 0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.blocks = nn.ModuleList([SSMBlock(emb_dim, state_size, dropout) for _ in range(num_layers)])\n",
    "        self.head = nn.Linear(emb_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.emb(input_ids)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        pooled = x.mean(dim=1)\n",
    "        return self.head(self.dropout(pooled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:03:14.102504Z",
     "iopub.status.busy": "2025-10-30T06:03:14.101660Z",
     "iopub.status.idle": "2025-10-30T06:03:14.115313Z",
     "shell.execute_reply": "2025-10-30T06:03:14.114257Z",
     "shell.execute_reply.started": "2025-10-30T06:03:14.102474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train_SSM(train_texts, train_labels, val_texts, val_labels,\n",
    "              num_classes, epochs, batch_size, max_len,\n",
    "              emb_dim, state_size, num_layers, lr,\n",
    "              device=\"cuda\"):\n",
    "\n",
    "    train_loader, val_loader, vocab = get_dataloaders(\n",
    "        train_texts, train_labels, val_texts, val_labels,\n",
    "        batch_size=batch_size, max_len=max_len\n",
    "    )\n",
    "\n",
    "    pad_idx = vocab.stoi[\"<pad>\"]\n",
    "    model = SSMTextClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_classes=num_classes,\n",
    "        emb_dim=emb_dim,\n",
    "        state_size=state_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=0.1,\n",
    "        pad_idx=pad_idx\n",
    "    ).to(device)\n",
    "\n",
    "    classes = np.unique(train_labels)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=classes,\n",
    "        y=train_labels\n",
    "    )\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    # --- training ---\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Train {epoch+1}/{epochs}\", unit=\"batch\")\n",
    "\n",
    "        for xb, yb in pbar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"acc\": f\"{(correct / total):.4f}\"\n",
    "            })\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            pbar_val = tqdm(val_loader, desc=f\"Valid {epoch+1}/{epochs}\", unit=\"batch\")\n",
    "            for xb, yb in pbar_val:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                with torch.amp.autocast(\"cuda\"):\n",
    "                    logits = model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                val_loss += loss.item()\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        val_acc = val_correct / val_total\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()    \n",
    "    plt.savefig('basic_loss_curve.png')\n",
    "\n",
    "    return model, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:03:16.621020Z",
     "iopub.status.busy": "2025-10-30T06:03:16.620239Z",
     "iopub.status.idle": "2025-10-30T06:03:16.626167Z",
     "shell.execute_reply": "2025-10-30T06:03:16.625153Z",
     "shell.execute_reply.started": "2025-10-30T06:03:16.620993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def main_SSM(\n",
    "    train_path,\n",
    "    valid_path,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    max_len,\n",
    "    emb_dim,\n",
    "    state_size,\n",
    "    num_layers,\n",
    "    lr\n",
    "):\n",
    "\n",
    "    train_texts, train_labels, val_texts, val_labels, _, _, num_classes = load_dbpedia_data(\n",
    "        train_path,\n",
    "        valid_path,\n",
    "        None\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    model, vocab = train_SSM(\n",
    "        train_texts, train_labels, val_texts, val_labels,\n",
    "        num_classes=num_classes,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        max_len=max_len,\n",
    "        emb_dim=emb_dim,\n",
    "        state_size=state_size,\n",
    "        num_layers=num_layers,\n",
    "        lr=lr\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return model, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/kaggle/input/dataset-llm/dataset/train.json\"\n",
    "valid_path = \"/kaggle/input/dataset-llm/dataset/val.json\"\n",
    "\n",
    "model, vocab = main_SSM(\n",
    "    train_path, valid_path,\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    max_len=64,\n",
    "    emb_dim=128,\n",
    "    state_size=32,\n",
    "    num_layers=3,\n",
    "    lr=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FFT-based Convolutional SSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:04:57.573765Z",
     "iopub.status.busy": "2025-10-30T06:04:57.573436Z",
     "iopub.status.idle": "2025-10-30T06:04:57.587163Z",
     "shell.execute_reply": "2025-10-30T06:04:57.586313Z",
     "shell.execute_reply.started": "2025-10-30T06:04:57.573740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- FFT-based Convolutional SSM (Simplified S4) ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FFTSSMBlock(nn.Module):\n",
    "    def __init__(self, seq_len: int, in_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "        #=== SSM parameters ===#\n",
    "        # A, B, C are all learnable\n",
    "        # A_raw is positive, then negated for stability\n",
    "        self.A_raw = nn.Parameter(torch.randn(in_dim))       # shape (D,)\n",
    "        self.B = nn.Parameter(torch.randn(in_dim))           # shape (D,)\n",
    "        self.C = nn.Parameter(torch.randn(in_dim))           # shape (D,)\n",
    "\n",
    "        #=== Projections ===#\n",
    "        self.proj_in = nn.Linear(in_dim, in_dim)\n",
    "        self.proj_out = nn.Linear(in_dim, in_dim)\n",
    "\n",
    "        self.norm = nn.LayerNorm(in_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gate = nn.Sigmoid()\n",
    "\n",
    "    def compute_kernel(self, L):\n",
    "        \"\"\"\n",
    "        Compute convolution kernel K of shape (in_dim, L)\n",
    "        Using A, B, C with exponential decay (S4D-like)\n",
    "        \"\"\"\n",
    "        t = torch.arange(L, device=self.A_raw.device, dtype=torch.float32)  # (L,)\n",
    "        A = -F.softplus(self.A_raw)  # ensure stability: A < 0\n",
    "        decay = torch.exp(torch.outer(t, A))  # (L, D)\n",
    "        K = decay * (self.C * self.B) * 0.1   # (L, D)\n",
    "        return K.T  # -> (D, L)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, L, D)\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        residual = x\n",
    "\n",
    "        #=== input projection ===#\n",
    "        u = self.proj_in(x)  # (B, L, D)\n",
    "\n",
    "        #=== SSM Kernel ===#\n",
    "        K = self.compute_kernel(L)  # (D, L)\n",
    "\n",
    "        #=== FFT convolution ===#\n",
    "        # We convolve each channel separately\n",
    "        # u: (B, D, L)\n",
    "        u_f = torch.fft.rfft(u.transpose(1, 2), n=2*L)   # (B, D, ?)\n",
    "        k_f = torch.fft.rfft(K, n=2*L)                   # (D, ?)\n",
    "\n",
    "        # Multiply in frequency domain\n",
    "        y_f = u_f * k_f.unsqueeze(0)                     # (B, D, ?)\n",
    "\n",
    "        # Inverse FFT to time domain\n",
    "        y = torch.fft.irfft(y_f, n=2*L)[..., :L]         # (B, D, L)\n",
    "        y = y.transpose(1, 2)                            # (B, L, D)\n",
    "\n",
    "        #=== output projection + gate ===#\n",
    "        y = self.proj_out(y)\n",
    "        y = self.gate(y) * y\n",
    "        y = self.dropout(y)\n",
    "\n",
    "        return self.norm(residual + y)\n",
    "\n",
    "class S4TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size: int, num_classes: int, seq_len: int,\n",
    "                 emb_dim=128, num_layers=2, dropout=0.1, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            FFTSSMBlock(seq_len, emb_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.head = nn.Linear(emb_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # === embedding ===\n",
    "        x = self.emb(input_ids)\n",
    "\n",
    "        # === stacked SSM blocks ===\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        # === pooling & classifier head ===\n",
    "        pooled = x.mean(dim=1)\n",
    "        return self.head(self.dropout(pooled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:05:01.058128Z",
     "iopub.status.busy": "2025-10-30T06:05:01.057535Z",
     "iopub.status.idle": "2025-10-30T06:05:01.070798Z",
     "shell.execute_reply": "2025-10-30T06:05:01.069901Z",
     "shell.execute_reply.started": "2025-10-30T06:05:01.058104Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def train_S4(\n",
    "    train_texts, train_labels,\n",
    "    val_texts, val_labels,\n",
    "    num_classes,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    max_len=128,\n",
    "    emb_dim=256,\n",
    "    num_layers=4,\n",
    "    lr=1e-3,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "\n",
    "    train_loader, val_loader, vocab = get_dataloaders(\n",
    "        train_texts, train_labels, val_texts, val_labels,\n",
    "        batch_size=batch_size, max_len=max_len\n",
    "    )\n",
    "\n",
    "    pad_idx = vocab.stoi[\"<pad>\"]\n",
    "    model = S4TextClassifier(\n",
    "        vocab_size=len(vocab),\n",
    "        num_classes=num_classes,\n",
    "        seq_len=max_len,\n",
    "        emb_dim=emb_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=0.1,\n",
    "        pad_idx=pad_idx\n",
    "    ).to(device)\n",
    "\n",
    "    classes = np.unique(train_labels)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=classes,\n",
    "        y=train_labels\n",
    "    )\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Train {epoch+1}/{epochs}\", unit=\"batch\")\n",
    "\n",
    "        for xb, yb in pbar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"acc\": f\"{(correct / total):.4f}\"\n",
    "            })\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            pbar_val = tqdm(val_loader, desc=f\"Valid {epoch+1}/{epochs}\", unit=\"batch\")\n",
    "            for xb, yb in pbar_val:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                with torch.amp.autocast(\"cuda\"):\n",
    "                    logits = model(xb)\n",
    "                    loss = criterion(logits, yb)\n",
    "                val_loss += loss.item()\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        val_acc = val_correct / val_total\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()    \n",
    "    plt.savefig('s4_loss_curve.png')\n",
    "\n",
    "    return model, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T06:05:04.691074Z",
     "iopub.status.busy": "2025-10-30T06:05:04.690430Z",
     "iopub.status.idle": "2025-10-30T06:05:04.696341Z",
     "shell.execute_reply": "2025-10-30T06:05:04.695380Z",
     "shell.execute_reply.started": "2025-10-30T06:05:04.691050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main_S4(\n",
    "    train_path,\n",
    "    val_path,\n",
    "    test_path,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    max_len,\n",
    "    emb_dim,\n",
    "    num_layers,\n",
    "    lr\n",
    "):\n",
    "\n",
    "    train_texts, train_labels, val_texts, val_labels, test_texts, test_ids, num_classes = load_dbpedia_data(\n",
    "        train_path, val_path, test_path\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model, vocab = train_S4(\n",
    "        train_texts, train_labels,\n",
    "        val_texts, val_labels,\n",
    "        num_classes=num_classes,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        max_len=max_len,\n",
    "        emb_dim=emb_dim,\n",
    "        num_layers=num_layers,\n",
    "        lr=lr\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"Training time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    return model, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/kaggle/input/dataset-llm/dataset/train.json\"\n",
    "valid_path = \"/kaggle/input/dataset-llm/dataset/val.json\"\n",
    "\n",
    "model, vocab = main_S4(\n",
    "    train_path, valid_path, None,\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    max_len=64,\n",
    "    emb_dim=128,\n",
    "    num_layers=3,\n",
    "    lr=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test.json\n",
    "test_path = \"/kaggle/input/dataset-llm/test.json\"\n",
    "\n",
    "def predict_test(model, vocab, test_path, batch_size=32, max_len=128, device=\"cuda\"):\n",
    "    test_texts, test_ids = load_dbpedia_data(train_path, valid_path, test_path)[4:6]\n",
    "    test_loader = get_test_loader(test_texts, vocab, batch_size=batch_size, max_len=max_len)\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for xb in tqdm(test_loader, desc=\"Predicting\", unit=\"batch\"):\n",
    "            xb = xb.to(device)\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                logits = model(xb)\n",
    "            preds = logits.argmax(dim=1).cpu().tolist()\n",
    "            predictions.extend(preds)\n",
    "\n",
    "    return list(zip(test_ids, predictions))\n",
    "\n",
    "preds = predict_test(model, vocab, test_path, batch_size=16, max_len=64, device=\"cuda\")\n",
    "\n",
    "with open(\"submission.csv\", \"w\") as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for idx, label in preds:\n",
    "        f.write(f\"{idx},{label}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8607229,
     "sourceId": 13552089,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
